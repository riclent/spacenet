__BRYTHON__.use_VFS = true;
var scripts = {"$timestamp": 1710467007051, "_yaml": [".py", "\n\n\n\n\nimport yaml\n\n\n\nif not getattr(yaml,'__with_libyaml__',False ):\n from sys import version_info\n \n exc=ModuleNotFoundError if version_info >=(3,6)else ImportError\n raise exc(\"No module named '_yaml'\")\nelse :\n from yaml._yaml import *\n import warnings\n warnings.warn(\n 'The _yaml extension module is now located at yaml._yaml'\n ' and its location is subject to change.  To use the'\n ' LibYAML-based parser and emitter, import from `yaml`:'\n ' `from yaml import CLoader as Loader, CDumper as Dumper`.',\n DeprecationWarning\n )\n del warnings\n \n \n \n__name__='_yaml'\n\n\n\n__package__=''\n", ["sys", "warnings", "yaml", "yaml._yaml"], 1], "yaml.scanner": [".py", "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n__all__=['Scanner','ScannerError']\n\nfrom .error import MarkedYAMLError\nfrom .tokens import *\n\nclass ScannerError(MarkedYAMLError):\n pass\n \nclass SimpleKey:\n\n\n def __init__(self,token_number,required,index,line,column,mark):\n  self.token_number=token_number\n  self.required=required\n  self.index=index\n  self.line=line\n  self.column=column\n  self.mark=mark\n  \nclass Scanner:\n\n def __init__(self):\n  ''\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  self.done=False\n  \n  \n  \n  self.flow_level=0\n  \n  \n  self.tokens=[]\n  \n  \n  self.fetch_stream_start()\n  \n  \n  self.tokens_taken=0\n  \n  \n  self.indent=-1\n  \n  \n  self.indents=[]\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  self.allow_simple_key=True\n  \n  \n  \n  \n  \n  \n  \n  self.possible_simple_keys={}\n  \n  \n  \n def check_token(self,*choices):\n \n  while self.need_more_tokens():\n   self.fetch_more_tokens()\n  if self.tokens:\n   if not choices:\n    return True\n   for choice in choices:\n    if isinstance(self.tokens[0],choice):\n     return True\n  return False\n  \n def peek_token(self):\n \n \n  while self.need_more_tokens():\n   self.fetch_more_tokens()\n  if self.tokens:\n   return self.tokens[0]\n  else :\n   return None\n   \n def get_token(self):\n \n  while self.need_more_tokens():\n   self.fetch_more_tokens()\n  if self.tokens:\n   self.tokens_taken +=1\n   return self.tokens.pop(0)\n   \n   \n   \n def need_more_tokens(self):\n  if self.done:\n   return False\n  if not self.tokens:\n   return True\n   \n   \n  self.stale_possible_simple_keys()\n  if self.next_possible_simple_key()==self.tokens_taken:\n   return True\n   \n def fetch_more_tokens(self):\n \n \n  self.scan_to_next_token()\n  \n  \n  self.stale_possible_simple_keys()\n  \n  \n  \n  self.unwind_indent(self.column)\n  \n  \n  ch=self.peek()\n  \n  \n  if ch =='\\0':\n   return self.fetch_stream_end()\n   \n   \n  if ch =='%'and self.check_directive():\n   return self.fetch_directive()\n   \n   \n  if ch =='-'and self.check_document_start():\n   return self.fetch_document_start()\n   \n   \n  if ch =='.'and self.check_document_end():\n   return self.fetch_document_end()\n   \n   \n   \n   \n   \n   \n   \n   \n  if ch =='[':\n   return self.fetch_flow_sequence_start()\n   \n   \n  if ch =='{':\n   return self.fetch_flow_mapping_start()\n   \n   \n  if ch ==']':\n   return self.fetch_flow_sequence_end()\n   \n   \n  if ch =='}':\n   return self.fetch_flow_mapping_end()\n   \n   \n  if ch ==',':\n   return self.fetch_flow_entry()\n   \n   \n  if ch =='-'and self.check_block_entry():\n   return self.fetch_block_entry()\n   \n   \n  if ch =='?'and self.check_key():\n   return self.fetch_key()\n   \n   \n  if ch ==':'and self.check_value():\n   return self.fetch_value()\n   \n   \n  if ch =='*':\n   return self.fetch_alias()\n   \n   \n  if ch =='&':\n   return self.fetch_anchor()\n   \n   \n  if ch =='!':\n   return self.fetch_tag()\n   \n   \n  if ch =='|'and not self.flow_level:\n   return self.fetch_literal()\n   \n   \n  if ch =='>'and not self.flow_level:\n   return self.fetch_folded()\n   \n   \n  if ch =='\\'':\n   return self.fetch_single()\n   \n   \n  if ch =='\\\"':\n   return self.fetch_double()\n   \n   \n  if self.check_plain():\n   return self.fetch_plain()\n   \n   \n  raise ScannerError(\"while scanning for the next token\",None ,\n  \"found character %r that cannot start any token\"%ch,\n  self.get_mark())\n  \n  \n  \n def next_possible_simple_key(self):\n \n \n \n \n \n \n \n  min_token_number=None\n  for level in self.possible_simple_keys:\n   key=self.possible_simple_keys[level]\n   if min_token_number is None or key.token_number <min_token_number:\n    min_token_number=key.token_number\n  return min_token_number\n  \n def stale_possible_simple_keys(self):\n \n \n \n \n \n \n  for level in list(self.possible_simple_keys):\n   key=self.possible_simple_keys[level]\n   if key.line !=self.line\\\n   or self.index -key.index >1024:\n    if key.required:\n     raise ScannerError(\"while scanning a simple key\",key.mark,\n     \"could not find expected ':'\",self.get_mark())\n    del self.possible_simple_keys[level]\n    \n def save_possible_simple_key(self):\n \n \n \n \n \n  required=not self.flow_level and self.indent ==self.column\n  \n  \n  \n  if self.allow_simple_key:\n   self.remove_possible_simple_key()\n   token_number=self.tokens_taken+len(self.tokens)\n   key=SimpleKey(token_number,required,\n   self.index,self.line,self.column,self.get_mark())\n   self.possible_simple_keys[self.flow_level]=key\n   \n def remove_possible_simple_key(self):\n \n  if self.flow_level in self.possible_simple_keys:\n   key=self.possible_simple_keys[self.flow_level]\n   \n   if key.required:\n    raise ScannerError(\"while scanning a simple key\",key.mark,\n    \"could not find expected ':'\",self.get_mark())\n    \n   del self.possible_simple_keys[self.flow_level]\n   \n   \n   \n def unwind_indent(self,column):\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n  if self.flow_level:\n   return\n   \n   \n  while self.indent >column:\n   mark=self.get_mark()\n   self.indent=self.indents.pop()\n   self.tokens.append(BlockEndToken(mark,mark))\n   \n def add_indent(self,column):\n \n  if self.indent <column:\n   self.indents.append(self.indent)\n   self.indent=column\n   return True\n  return False\n  \n  \n  \n def fetch_stream_start(self):\n \n \n \n \n  mark=self.get_mark()\n  \n  \n  self.tokens.append(StreamStartToken(mark,mark,\n  encoding=self.encoding))\n  \n  \n def fetch_stream_end(self):\n \n \n  self.unwind_indent(-1)\n  \n  \n  self.remove_possible_simple_key()\n  self.allow_simple_key=False\n  self.possible_simple_keys={}\n  \n  \n  mark=self.get_mark()\n  \n  \n  self.tokens.append(StreamEndToken(mark,mark))\n  \n  \n  self.done=True\n  \n def fetch_directive(self):\n \n \n  self.unwind_indent(-1)\n  \n  \n  self.remove_possible_simple_key()\n  self.allow_simple_key=False\n  \n  \n  self.tokens.append(self.scan_directive())\n  \n def fetch_document_start(self):\n  self.fetch_document_indicator(DocumentStartToken)\n  \n def fetch_document_end(self):\n  self.fetch_document_indicator(DocumentEndToken)\n  \n def fetch_document_indicator(self,TokenClass):\n \n \n  self.unwind_indent(-1)\n  \n  \n  \n  self.remove_possible_simple_key()\n  self.allow_simple_key=False\n  \n  \n  start_mark=self.get_mark()\n  self.forward(3)\n  end_mark=self.get_mark()\n  self.tokens.append(TokenClass(start_mark,end_mark))\n  \n def fetch_flow_sequence_start(self):\n  self.fetch_flow_collection_start(FlowSequenceStartToken)\n  \n def fetch_flow_mapping_start(self):\n  self.fetch_flow_collection_start(FlowMappingStartToken)\n  \n def fetch_flow_collection_start(self,TokenClass):\n \n \n  self.save_possible_simple_key()\n  \n  \n  self.flow_level +=1\n  \n  \n  self.allow_simple_key=True\n  \n  \n  start_mark=self.get_mark()\n  self.forward()\n  end_mark=self.get_mark()\n  self.tokens.append(TokenClass(start_mark,end_mark))\n  \n def fetch_flow_sequence_end(self):\n  self.fetch_flow_collection_end(FlowSequenceEndToken)\n  \n def fetch_flow_mapping_end(self):\n  self.fetch_flow_collection_end(FlowMappingEndToken)\n  \n def fetch_flow_collection_end(self,TokenClass):\n \n \n  self.remove_possible_simple_key()\n  \n  \n  self.flow_level -=1\n  \n  \n  self.allow_simple_key=False\n  \n  \n  start_mark=self.get_mark()\n  self.forward()\n  end_mark=self.get_mark()\n  self.tokens.append(TokenClass(start_mark,end_mark))\n  \n def fetch_flow_entry(self):\n \n \n  self.allow_simple_key=True\n  \n  \n  self.remove_possible_simple_key()\n  \n  \n  start_mark=self.get_mark()\n  self.forward()\n  end_mark=self.get_mark()\n  self.tokens.append(FlowEntryToken(start_mark,end_mark))\n  \n def fetch_block_entry(self):\n \n \n  if not self.flow_level:\n  \n  \n   if not self.allow_simple_key:\n    raise ScannerError(None ,None ,\n    \"sequence entries are not allowed here\",\n    self.get_mark())\n    \n    \n   if self.add_indent(self.column):\n    mark=self.get_mark()\n    self.tokens.append(BlockSequenceStartToken(mark,mark))\n    \n    \n    \n  else :\n   pass\n   \n   \n  self.allow_simple_key=True\n  \n  \n  self.remove_possible_simple_key()\n  \n  \n  start_mark=self.get_mark()\n  self.forward()\n  end_mark=self.get_mark()\n  self.tokens.append(BlockEntryToken(start_mark,end_mark))\n  \n def fetch_key(self):\n \n \n  if not self.flow_level:\n  \n  \n   if not self.allow_simple_key:\n    raise ScannerError(None ,None ,\n    \"mapping keys are not allowed here\",\n    self.get_mark())\n    \n    \n   if self.add_indent(self.column):\n    mark=self.get_mark()\n    self.tokens.append(BlockMappingStartToken(mark,mark))\n    \n    \n  self.allow_simple_key=not self.flow_level\n  \n  \n  self.remove_possible_simple_key()\n  \n  \n  start_mark=self.get_mark()\n  self.forward()\n  end_mark=self.get_mark()\n  self.tokens.append(KeyToken(start_mark,end_mark))\n  \n def fetch_value(self):\n \n \n  if self.flow_level in self.possible_simple_keys:\n  \n  \n   key=self.possible_simple_keys[self.flow_level]\n   del self.possible_simple_keys[self.flow_level]\n   self.tokens.insert(key.token_number -self.tokens_taken,\n   KeyToken(key.mark,key.mark))\n   \n   \n   \n   if not self.flow_level:\n    if self.add_indent(key.column):\n     self.tokens.insert(key.token_number -self.tokens_taken,\n     BlockMappingStartToken(key.mark,key.mark))\n     \n     \n   self.allow_simple_key=False\n   \n   \n  else :\n  \n  \n  \n  \n   if not self.flow_level:\n   \n   \n   \n    if not self.allow_simple_key:\n     raise ScannerError(None ,None ,\n     \"mapping values are not allowed here\",\n     self.get_mark())\n     \n     \n     \n     \n   if not self.flow_level:\n    if self.add_indent(self.column):\n     mark=self.get_mark()\n     self.tokens.append(BlockMappingStartToken(mark,mark))\n     \n     \n   self.allow_simple_key=not self.flow_level\n   \n   \n   self.remove_possible_simple_key()\n   \n   \n  start_mark=self.get_mark()\n  self.forward()\n  end_mark=self.get_mark()\n  self.tokens.append(ValueToken(start_mark,end_mark))\n  \n def fetch_alias(self):\n \n \n  self.save_possible_simple_key()\n  \n  \n  self.allow_simple_key=False\n  \n  \n  self.tokens.append(self.scan_anchor(AliasToken))\n  \n def fetch_anchor(self):\n \n \n  self.save_possible_simple_key()\n  \n  \n  self.allow_simple_key=False\n  \n  \n  self.tokens.append(self.scan_anchor(AnchorToken))\n  \n def fetch_tag(self):\n \n \n  self.save_possible_simple_key()\n  \n  \n  self.allow_simple_key=False\n  \n  \n  self.tokens.append(self.scan_tag())\n  \n def fetch_literal(self):\n  self.fetch_block_scalar(style='|')\n  \n def fetch_folded(self):\n  self.fetch_block_scalar(style='>')\n  \n def fetch_block_scalar(self,style):\n \n \n  self.allow_simple_key=True\n  \n  \n  self.remove_possible_simple_key()\n  \n  \n  self.tokens.append(self.scan_block_scalar(style))\n  \n def fetch_single(self):\n  self.fetch_flow_scalar(style='\\'')\n  \n def fetch_double(self):\n  self.fetch_flow_scalar(style='\"')\n  \n def fetch_flow_scalar(self,style):\n \n \n  self.save_possible_simple_key()\n  \n  \n  self.allow_simple_key=False\n  \n  \n  self.tokens.append(self.scan_flow_scalar(style))\n  \n def fetch_plain(self):\n \n \n  self.save_possible_simple_key()\n  \n  \n  \n  \n  self.allow_simple_key=False\n  \n  \n  self.tokens.append(self.scan_plain())\n  \n  \n  \n def check_directive(self):\n \n \n \n  if self.column ==0:\n   return True\n   \n def check_document_start(self):\n \n \n  if self.column ==0:\n   if self.prefix(3)=='---'\\\n   and self.peek(3)in '\\0 \\t\\r\\n\\x85\\u2028\\u2029':\n    return True\n    \n def check_document_end(self):\n \n \n  if self.column ==0:\n   if self.prefix(3)=='...'\\\n   and self.peek(3)in '\\0 \\t\\r\\n\\x85\\u2028\\u2029':\n    return True\n    \n def check_block_entry(self):\n \n \n  return self.peek(1)in '\\0 \\t\\r\\n\\x85\\u2028\\u2029'\n  \n def check_key(self):\n \n \n  if self.flow_level:\n   return True\n   \n   \n  else :\n   return self.peek(1)in '\\0 \\t\\r\\n\\x85\\u2028\\u2029'\n   \n def check_value(self):\n \n \n  if self.flow_level:\n   return True\n   \n   \n  else :\n   return self.peek(1)in '\\0 \\t\\r\\n\\x85\\u2028\\u2029'\n   \n def check_plain(self):\n \n \n \n \n \n \n \n \n \n \n \n \n \n  ch=self.peek()\n  return ch not in '\\0 \\t\\r\\n\\x85\\u2028\\u2029-?:,[]{}#&*!|>\\'\\\"%@`'\\\n  or (self.peek(1)not in '\\0 \\t\\r\\n\\x85\\u2028\\u2029'\n  and (ch =='-'or (not self.flow_level and ch in '?:')))\n  \n  \n  \n def scan_to_next_token(self):\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n  if self.index ==0 and self.peek()=='\\uFEFF':\n   self.forward()\n  found=False\n  while not found:\n   while self.peek()==' ':\n    self.forward()\n   if self.peek()=='#':\n    while self.peek()not in '\\0\\r\\n\\x85\\u2028\\u2029':\n     self.forward()\n   if self.scan_line_break():\n    if not self.flow_level:\n     self.allow_simple_key=True\n   else :\n    found=True\n    \n def scan_directive(self):\n \n  start_mark=self.get_mark()\n  self.forward()\n  name=self.scan_directive_name(start_mark)\n  value=None\n  if name =='YAML':\n   value=self.scan_yaml_directive_value(start_mark)\n   end_mark=self.get_mark()\n  elif name =='TAG':\n   value=self.scan_tag_directive_value(start_mark)\n   end_mark=self.get_mark()\n  else :\n   end_mark=self.get_mark()\n   while self.peek()not in '\\0\\r\\n\\x85\\u2028\\u2029':\n    self.forward()\n  self.scan_directive_ignored_line(start_mark)\n  return DirectiveToken(name,value,start_mark,end_mark)\n  \n def scan_directive_name(self,start_mark):\n \n  length=0\n  ch=self.peek(length)\n  while '0'<=ch <='9'or 'A'<=ch <='Z'or 'a'<=ch <='z'\\\n  or ch in '-_':\n   length +=1\n   ch=self.peek(length)\n  if not length:\n   raise ScannerError(\"while scanning a directive\",start_mark,\n   \"expected alphabetic or numeric character, but found %r\"\n   %ch,self.get_mark())\n  value=self.prefix(length)\n  self.forward(length)\n  ch=self.peek()\n  if ch not in '\\0 \\r\\n\\x85\\u2028\\u2029':\n   raise ScannerError(\"while scanning a directive\",start_mark,\n   \"expected alphabetic or numeric character, but found %r\"\n   %ch,self.get_mark())\n  return value\n  \n def scan_yaml_directive_value(self,start_mark):\n \n  while self.peek()==' ':\n   self.forward()\n  major=self.scan_yaml_directive_number(start_mark)\n  if self.peek()!='.':\n   raise ScannerError(\"while scanning a directive\",start_mark,\n   \"expected a digit or '.', but found %r\"%self.peek(),\n   self.get_mark())\n  self.forward()\n  minor=self.scan_yaml_directive_number(start_mark)\n  if self.peek()not in '\\0 \\r\\n\\x85\\u2028\\u2029':\n   raise ScannerError(\"while scanning a directive\",start_mark,\n   \"expected a digit or ' ', but found %r\"%self.peek(),\n   self.get_mark())\n  return (major,minor)\n  \n def scan_yaml_directive_number(self,start_mark):\n \n  ch=self.peek()\n  if not ('0'<=ch <='9'):\n   raise ScannerError(\"while scanning a directive\",start_mark,\n   \"expected a digit, but found %r\"%ch,self.get_mark())\n  length=0\n  while '0'<=self.peek(length)<='9':\n   length +=1\n  value=int(self.prefix(length))\n  self.forward(length)\n  return value\n  \n def scan_tag_directive_value(self,start_mark):\n \n  while self.peek()==' ':\n   self.forward()\n  handle=self.scan_tag_directive_handle(start_mark)\n  while self.peek()==' ':\n   self.forward()\n  prefix=self.scan_tag_directive_prefix(start_mark)\n  return (handle,prefix)\n  \n def scan_tag_directive_handle(self,start_mark):\n \n  value=self.scan_tag_handle('directive',start_mark)\n  ch=self.peek()\n  if ch !=' ':\n   raise ScannerError(\"while scanning a directive\",start_mark,\n   \"expected ' ', but found %r\"%ch,self.get_mark())\n  return value\n  \n def scan_tag_directive_prefix(self,start_mark):\n \n  value=self.scan_tag_uri('directive',start_mark)\n  ch=self.peek()\n  if ch not in '\\0 \\r\\n\\x85\\u2028\\u2029':\n   raise ScannerError(\"while scanning a directive\",start_mark,\n   \"expected ' ', but found %r\"%ch,self.get_mark())\n  return value\n  \n def scan_directive_ignored_line(self,start_mark):\n \n  while self.peek()==' ':\n   self.forward()\n  if self.peek()=='#':\n   while self.peek()not in '\\0\\r\\n\\x85\\u2028\\u2029':\n    self.forward()\n  ch=self.peek()\n  if ch not in '\\0\\r\\n\\x85\\u2028\\u2029':\n   raise ScannerError(\"while scanning a directive\",start_mark,\n   \"expected a comment or a line break, but found %r\"\n   %ch,self.get_mark())\n  self.scan_line_break()\n  \n def scan_anchor(self,TokenClass):\n \n \n \n \n \n \n \n \n  start_mark=self.get_mark()\n  indicator=self.peek()\n  if indicator =='*':\n   name='alias'\n  else :\n   name='anchor'\n  self.forward()\n  length=0\n  ch=self.peek(length)\n  while '0'<=ch <='9'or 'A'<=ch <='Z'or 'a'<=ch <='z'\\\n  or ch in '-_':\n   length +=1\n   ch=self.peek(length)\n  if not length:\n   raise ScannerError(\"while scanning an %s\"%name,start_mark,\n   \"expected alphabetic or numeric character, but found %r\"\n   %ch,self.get_mark())\n  value=self.prefix(length)\n  self.forward(length)\n  ch=self.peek()\n  if ch not in '\\0 \\t\\r\\n\\x85\\u2028\\u2029?:,]}%@`':\n   raise ScannerError(\"while scanning an %s\"%name,start_mark,\n   \"expected alphabetic or numeric character, but found %r\"\n   %ch,self.get_mark())\n  end_mark=self.get_mark()\n  return TokenClass(value,start_mark,end_mark)\n  \n def scan_tag(self):\n \n  start_mark=self.get_mark()\n  ch=self.peek(1)\n  if ch =='<':\n   handle=None\n   self.forward(2)\n   suffix=self.scan_tag_uri('tag',start_mark)\n   if self.peek()!='>':\n    raise ScannerError(\"while parsing a tag\",start_mark,\n    \"expected '>', but found %r\"%self.peek(),\n    self.get_mark())\n   self.forward()\n  elif ch in '\\0 \\t\\r\\n\\x85\\u2028\\u2029':\n   handle=None\n   suffix='!'\n   self.forward()\n  else :\n   length=1\n   use_handle=False\n   while ch not in '\\0 \\r\\n\\x85\\u2028\\u2029':\n    if ch =='!':\n     use_handle=True\n     break\n    length +=1\n    ch=self.peek(length)\n   handle='!'\n   if use_handle:\n    handle=self.scan_tag_handle('tag',start_mark)\n   else :\n    handle='!'\n    self.forward()\n   suffix=self.scan_tag_uri('tag',start_mark)\n  ch=self.peek()\n  if ch not in '\\0 \\r\\n\\x85\\u2028\\u2029':\n   raise ScannerError(\"while scanning a tag\",start_mark,\n   \"expected ' ', but found %r\"%ch,self.get_mark())\n  value=(handle,suffix)\n  end_mark=self.get_mark()\n  return TagToken(value,start_mark,end_mark)\n  \n def scan_block_scalar(self,style):\n \n \n  if style =='>':\n   folded=True\n  else :\n   folded=False\n   \n  chunks=[]\n  start_mark=self.get_mark()\n  \n  \n  self.forward()\n  chomping,increment=self.scan_block_scalar_indicators(start_mark)\n  self.scan_block_scalar_ignored_line(start_mark)\n  \n  \n  min_indent=self.indent+1\n  if min_indent <1:\n   min_indent=1\n  if increment is None :\n   breaks,max_indent,end_mark=self.scan_block_scalar_indentation()\n   indent=max(min_indent,max_indent)\n  else :\n   indent=min_indent+increment -1\n   breaks,end_mark=self.scan_block_scalar_breaks(indent)\n  line_break=''\n  \n  \n  while self.column ==indent and self.peek()!='\\0':\n   chunks.extend(breaks)\n   leading_non_space=self.peek()not in ' \\t'\n   length=0\n   while self.peek(length)not in '\\0\\r\\n\\x85\\u2028\\u2029':\n    length +=1\n   chunks.append(self.prefix(length))\n   self.forward(length)\n   line_break=self.scan_line_break()\n   breaks,end_mark=self.scan_block_scalar_breaks(indent)\n   if self.column ==indent and self.peek()!='\\0':\n   \n   \n   \n   \n   \n    if folded and line_break =='\\n'\\\n    and leading_non_space and self.peek()not in ' \\t':\n     if not breaks:\n      chunks.append(' ')\n    else :\n     chunks.append(line_break)\n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n   else :\n    break\n    \n    \n  if chomping is not False :\n   chunks.append(line_break)\n  if chomping is True :\n   chunks.extend(breaks)\n   \n   \n  return ScalarToken(''.join(chunks),False ,start_mark,end_mark,\n  style)\n  \n def scan_block_scalar_indicators(self,start_mark):\n \n  chomping=None\n  increment=None\n  ch=self.peek()\n  if ch in '+-':\n   if ch =='+':\n    chomping=True\n   else :\n    chomping=False\n   self.forward()\n   ch=self.peek()\n   if ch in '0123456789':\n    increment=int(ch)\n    if increment ==0:\n     raise ScannerError(\"while scanning a block scalar\",start_mark,\n     \"expected indentation indicator in the range 1-9, but found 0\",\n     self.get_mark())\n    self.forward()\n  elif ch in '0123456789':\n   increment=int(ch)\n   if increment ==0:\n    raise ScannerError(\"while scanning a block scalar\",start_mark,\n    \"expected indentation indicator in the range 1-9, but found 0\",\n    self.get_mark())\n   self.forward()\n   ch=self.peek()\n   if ch in '+-':\n    if ch =='+':\n     chomping=True\n    else :\n     chomping=False\n    self.forward()\n  ch=self.peek()\n  if ch not in '\\0 \\r\\n\\x85\\u2028\\u2029':\n   raise ScannerError(\"while scanning a block scalar\",start_mark,\n   \"expected chomping or indentation indicators, but found %r\"\n   %ch,self.get_mark())\n  return chomping,increment\n  \n def scan_block_scalar_ignored_line(self,start_mark):\n \n  while self.peek()==' ':\n   self.forward()\n  if self.peek()=='#':\n   while self.peek()not in '\\0\\r\\n\\x85\\u2028\\u2029':\n    self.forward()\n  ch=self.peek()\n  if ch not in '\\0\\r\\n\\x85\\u2028\\u2029':\n   raise ScannerError(\"while scanning a block scalar\",start_mark,\n   \"expected a comment or a line break, but found %r\"%ch,\n   self.get_mark())\n  self.scan_line_break()\n  \n def scan_block_scalar_indentation(self):\n \n  chunks=[]\n  max_indent=0\n  end_mark=self.get_mark()\n  while self.peek()in ' \\r\\n\\x85\\u2028\\u2029':\n   if self.peek()!=' ':\n    chunks.append(self.scan_line_break())\n    end_mark=self.get_mark()\n   else :\n    self.forward()\n    if self.column >max_indent:\n     max_indent=self.column\n  return chunks,max_indent,end_mark\n  \n def scan_block_scalar_breaks(self,indent):\n \n  chunks=[]\n  end_mark=self.get_mark()\n  while self.column <indent and self.peek()==' ':\n   self.forward()\n  while self.peek()in '\\r\\n\\x85\\u2028\\u2029':\n   chunks.append(self.scan_line_break())\n   end_mark=self.get_mark()\n   while self.column <indent and self.peek()==' ':\n    self.forward()\n  return chunks,end_mark\n  \n def scan_flow_scalar(self,style):\n \n \n \n \n \n \n  if style =='\"':\n   double=True\n  else :\n   double=False\n  chunks=[]\n  start_mark=self.get_mark()\n  quote=self.peek()\n  self.forward()\n  chunks.extend(self.scan_flow_scalar_non_spaces(double,start_mark))\n  while self.peek()!=quote:\n   chunks.extend(self.scan_flow_scalar_spaces(double,start_mark))\n   chunks.extend(self.scan_flow_scalar_non_spaces(double,start_mark))\n  self.forward()\n  end_mark=self.get_mark()\n  return ScalarToken(''.join(chunks),False ,start_mark,end_mark,\n  style)\n  \n ESCAPE_REPLACEMENTS={\n '0':'\\0',\n 'a':'\\x07',\n 'b':'\\x08',\n 't':'\\x09',\n '\\t':'\\x09',\n 'n':'\\x0A',\n 'v':'\\x0B',\n 'f':'\\x0C',\n 'r':'\\x0D',\n 'e':'\\x1B',\n ' ':'\\x20',\n '\\\"':'\\\"',\n '\\\\':'\\\\',\n '/':'/',\n 'N':'\\x85',\n '_':'\\xA0',\n 'L':'\\u2028',\n 'P':'\\u2029',\n }\n \n ESCAPE_CODES={\n 'x':2,\n 'u':4,\n 'U':8,\n }\n \n def scan_flow_scalar_non_spaces(self,double,start_mark):\n \n  chunks=[]\n  while True :\n   length=0\n   while self.peek(length)not in '\\'\\\"\\\\\\0 \\t\\r\\n\\x85\\u2028\\u2029':\n    length +=1\n   if length:\n    chunks.append(self.prefix(length))\n    self.forward(length)\n   ch=self.peek()\n   if not double and ch =='\\''and self.peek(1)=='\\'':\n    chunks.append('\\'')\n    self.forward(2)\n   elif (double and ch =='\\'')or (not double and ch in '\\\"\\\\'):\n    chunks.append(ch)\n    self.forward()\n   elif double and ch =='\\\\':\n    self.forward()\n    ch=self.peek()\n    if ch in self.ESCAPE_REPLACEMENTS:\n     chunks.append(self.ESCAPE_REPLACEMENTS[ch])\n     self.forward()\n    elif ch in self.ESCAPE_CODES:\n     length=self.ESCAPE_CODES[ch]\n     self.forward()\n     for k in range(length):\n      if self.peek(k)not in '0123456789ABCDEFabcdef':\n       raise ScannerError(\"while scanning a double-quoted scalar\",start_mark,\n       \"expected escape sequence of %d hexadecimal numbers, but found %r\"%\n       (length,self.peek(k)),self.get_mark())\n     code=int(self.prefix(length),16)\n     chunks.append(chr(code))\n     self.forward(length)\n    elif ch in '\\r\\n\\x85\\u2028\\u2029':\n     self.scan_line_break()\n     chunks.extend(self.scan_flow_scalar_breaks(double,start_mark))\n    else :\n     raise ScannerError(\"while scanning a double-quoted scalar\",start_mark,\n     \"found unknown escape character %r\"%ch,self.get_mark())\n   else :\n    return chunks\n    \n def scan_flow_scalar_spaces(self,double,start_mark):\n \n  chunks=[]\n  length=0\n  while self.peek(length)in ' \\t':\n   length +=1\n  whitespaces=self.prefix(length)\n  self.forward(length)\n  ch=self.peek()\n  if ch =='\\0':\n   raise ScannerError(\"while scanning a quoted scalar\",start_mark,\n   \"found unexpected end of stream\",self.get_mark())\n  elif ch in '\\r\\n\\x85\\u2028\\u2029':\n   line_break=self.scan_line_break()\n   breaks=self.scan_flow_scalar_breaks(double,start_mark)\n   if line_break !='\\n':\n    chunks.append(line_break)\n   elif not breaks:\n    chunks.append(' ')\n   chunks.extend(breaks)\n  else :\n   chunks.append(whitespaces)\n  return chunks\n  \n def scan_flow_scalar_breaks(self,double,start_mark):\n \n  chunks=[]\n  while True :\n  \n  \n   prefix=self.prefix(3)\n   if (prefix =='---'or prefix =='...')\\\n   and self.peek(3)in '\\0 \\t\\r\\n\\x85\\u2028\\u2029':\n    raise ScannerError(\"while scanning a quoted scalar\",start_mark,\n    \"found unexpected document separator\",self.get_mark())\n   while self.peek()in ' \\t':\n    self.forward()\n   if self.peek()in '\\r\\n\\x85\\u2028\\u2029':\n    chunks.append(self.scan_line_break())\n   else :\n    return chunks\n    \n def scan_plain(self):\n \n \n \n \n \n  chunks=[]\n  start_mark=self.get_mark()\n  end_mark=start_mark\n  indent=self.indent+1\n  \n  \n  \n  \n  spaces=[]\n  while True :\n   length=0\n   if self.peek()=='#':\n    break\n   while True :\n    ch=self.peek(length)\n    if ch in '\\0 \\t\\r\\n\\x85\\u2028\\u2029'\\\n    or (ch ==':'and\n    self.peek(length+1)in '\\0 \\t\\r\\n\\x85\\u2028\\u2029'\n    +(u',[]{}'if self.flow_level else u''))\\\n    or (self.flow_level and ch in ',?[]{}'):\n     break\n    length +=1\n   if length ==0:\n    break\n   self.allow_simple_key=False\n   chunks.extend(spaces)\n   chunks.append(self.prefix(length))\n   self.forward(length)\n   end_mark=self.get_mark()\n   spaces=self.scan_plain_spaces(indent,start_mark)\n   if not spaces or self.peek()=='#'\\\n   or (not self.flow_level and self.column <indent):\n    break\n  return ScalarToken(''.join(chunks),True ,start_mark,end_mark)\n  \n def scan_plain_spaces(self,indent,start_mark):\n \n \n \n  chunks=[]\n  length=0\n  while self.peek(length)in ' ':\n   length +=1\n  whitespaces=self.prefix(length)\n  self.forward(length)\n  ch=self.peek()\n  if ch in '\\r\\n\\x85\\u2028\\u2029':\n   line_break=self.scan_line_break()\n   self.allow_simple_key=True\n   prefix=self.prefix(3)\n   if (prefix =='---'or prefix =='...')\\\n   and self.peek(3)in '\\0 \\t\\r\\n\\x85\\u2028\\u2029':\n    return\n   breaks=[]\n   while self.peek()in ' \\r\\n\\x85\\u2028\\u2029':\n    if self.peek()==' ':\n     self.forward()\n    else :\n     breaks.append(self.scan_line_break())\n     prefix=self.prefix(3)\n     if (prefix =='---'or prefix =='...')\\\n     and self.peek(3)in '\\0 \\t\\r\\n\\x85\\u2028\\u2029':\n      return\n   if line_break !='\\n':\n    chunks.append(line_break)\n   elif not breaks:\n    chunks.append(' ')\n   chunks.extend(breaks)\n  elif whitespaces:\n   chunks.append(whitespaces)\n  return chunks\n  \n def scan_tag_handle(self,name,start_mark):\n \n \n \n  ch=self.peek()\n  if ch !='!':\n   raise ScannerError(\"while scanning a %s\"%name,start_mark,\n   \"expected '!', but found %r\"%ch,self.get_mark())\n  length=1\n  ch=self.peek(length)\n  if ch !=' ':\n   while '0'<=ch <='9'or 'A'<=ch <='Z'or 'a'<=ch <='z'\\\n   or ch in '-_':\n    length +=1\n    ch=self.peek(length)\n   if ch !='!':\n    self.forward(length)\n    raise ScannerError(\"while scanning a %s\"%name,start_mark,\n    \"expected '!', but found %r\"%ch,self.get_mark())\n   length +=1\n  value=self.prefix(length)\n  self.forward(length)\n  return value\n  \n def scan_tag_uri(self,name,start_mark):\n \n \n  chunks=[]\n  length=0\n  ch=self.peek(length)\n  while '0'<=ch <='9'or 'A'<=ch <='Z'or 'a'<=ch <='z'\\\n  or ch in '-;/?:@&=+$,_.!~*\\'()[]%':\n   if ch =='%':\n    chunks.append(self.prefix(length))\n    self.forward(length)\n    length=0\n    chunks.append(self.scan_uri_escapes(name,start_mark))\n   else :\n    length +=1\n   ch=self.peek(length)\n  if length:\n   chunks.append(self.prefix(length))\n   self.forward(length)\n   length=0\n  if not chunks:\n   raise ScannerError(\"while parsing a %s\"%name,start_mark,\n   \"expected URI, but found %r\"%ch,self.get_mark())\n  return ''.join(chunks)\n  \n def scan_uri_escapes(self,name,start_mark):\n \n  codes=[]\n  mark=self.get_mark()\n  while self.peek()=='%':\n   self.forward()\n   for k in range(2):\n    if self.peek(k)not in '0123456789ABCDEFabcdef':\n     raise ScannerError(\"while scanning a %s\"%name,start_mark,\n     \"expected URI escape sequence of 2 hexadecimal numbers, but found %r\"\n     %self.peek(k),self.get_mark())\n   codes.append(int(self.prefix(2),16))\n   self.forward(2)\n  try :\n   value=bytes(codes).decode('utf-8')\n  except UnicodeDecodeError as exc:\n   raise ScannerError(\"while scanning a %s\"%name,start_mark,str(exc),mark)\n  return value\n  \n def scan_line_break(self):\n \n \n \n \n \n \n \n \n  ch=self.peek()\n  if ch in '\\r\\n\\x85':\n   if self.prefix(2)=='\\r\\n':\n    self.forward(2)\n   else :\n    self.forward()\n   return '\\n'\n  elif ch in '\\u2028\\u2029':\n   self.forward()\n   return ch\n  return ''\n", ["yaml.error", "yaml.tokens"]], "yaml.error": [".py", "\n__all__=['Mark','YAMLError','MarkedYAMLError']\n\nclass Mark:\n\n def __init__(self,name,index,line,column,buffer,pointer):\n  self.name=name\n  self.index=index\n  self.line=line\n  self.column=column\n  self.buffer=buffer\n  self.pointer=pointer\n  \n def get_snippet(self,indent=4,max_length=75):\n  if self.buffer is None :\n   return None\n  head=''\n  start=self.pointer\n  while start >0 and self.buffer[start -1]not in '\\0\\r\\n\\x85\\u2028\\u2029':\n   start -=1\n   if self.pointer -start >max_length /2 -1:\n    head=' ... '\n    start +=5\n    break\n  tail=''\n  end=self.pointer\n  while end <len(self.buffer)and self.buffer[end]not in '\\0\\r\\n\\x85\\u2028\\u2029':\n   end +=1\n   if end -self.pointer >max_length /2 -1:\n    tail=' ... '\n    end -=5\n    break\n  snippet=self.buffer[start:end]\n  return ' '*indent+head+snippet+tail+'\\n'\\\n  +' '*(indent+self.pointer -start+len(head))+'^'\n  \n def __str__(self):\n  snippet=self.get_snippet()\n  where=\"  in \\\"%s\\\", line %d, column %d\"\\\n  %(self.name,self.line+1,self.column+1)\n  if snippet is not None :\n   where +=\":\\n\"+snippet\n  return where\n  \nclass YAMLError(Exception):\n pass\n \nclass MarkedYAMLError(YAMLError):\n\n def __init__(self,context=None ,context_mark=None ,\n problem=None ,problem_mark=None ,note=None ):\n  self.context=context\n  self.context_mark=context_mark\n  self.problem=problem\n  self.problem_mark=problem_mark\n  self.note=note\n  \n def __str__(self):\n  lines=[]\n  if self.context is not None :\n   lines.append(self.context)\n  if self.context_mark is not None\\\n  and (self.problem is None or self.problem_mark is None\n  or self.context_mark.name !=self.problem_mark.name\n  or self.context_mark.line !=self.problem_mark.line\n  or self.context_mark.column !=self.problem_mark.column):\n   lines.append(str(self.context_mark))\n  if self.problem is not None :\n   lines.append(self.problem)\n  if self.problem_mark is not None :\n   lines.append(str(self.problem_mark))\n  if self.note is not None :\n   lines.append(self.note)\n  return '\\n'.join(lines)\n  \n", []], "yaml.constructor": [".py", "\n__all__=[\n'BaseConstructor',\n'SafeConstructor',\n'FullConstructor',\n'UnsafeConstructor',\n'Constructor',\n'ConstructorError'\n]\n\nfrom .error import *\nfrom .nodes import *\n\nimport collections.abc,datetime,base64,binascii,re,sys,types\n\nclass ConstructorError(MarkedYAMLError):\n pass\n \nclass BaseConstructor:\n\n yaml_constructors={}\n yaml_multi_constructors={}\n \n def __init__(self):\n  self.constructed_objects={}\n  self.recursive_objects={}\n  self.state_generators=[]\n  self.deep_construct=False\n  \n def check_data(self):\n \n  return self.check_node()\n  \n def check_state_key(self,key):\n  ''\n\n  \n  if self.get_state_keys_blacklist_regexp().match(key):\n   raise ConstructorError(None ,None ,\n   \"blacklisted key '%s' in instance state found\"%(key,),None )\n   \n def get_data(self):\n \n  if self.check_node():\n   return self.construct_document(self.get_node())\n   \n def get_single_data(self):\n \n  node=self.get_single_node()\n  if node is not None :\n   return self.construct_document(node)\n  return None\n  \n def construct_document(self,node):\n  data=self.construct_object(node)\n  while self.state_generators:\n   state_generators=self.state_generators\n   self.state_generators=[]\n   for generator in state_generators:\n    for dummy in generator:\n     pass\n  self.constructed_objects={}\n  self.recursive_objects={}\n  self.deep_construct=False\n  return data\n  \n def construct_object(self,node,deep=False ):\n  if node in self.constructed_objects:\n   return self.constructed_objects[node]\n  if deep:\n   old_deep=self.deep_construct\n   self.deep_construct=True\n  if node in self.recursive_objects:\n   raise ConstructorError(None ,None ,\n   \"found unconstructable recursive node\",node.start_mark)\n  self.recursive_objects[node]=None\n  constructor=None\n  tag_suffix=None\n  if node.tag in self.yaml_constructors:\n   constructor=self.yaml_constructors[node.tag]\n  else :\n   for tag_prefix in self.yaml_multi_constructors:\n    if tag_prefix is not None and node.tag.startswith(tag_prefix):\n     tag_suffix=node.tag[len(tag_prefix):]\n     constructor=self.yaml_multi_constructors[tag_prefix]\n     break\n   else :\n    if None in self.yaml_multi_constructors:\n     tag_suffix=node.tag\n     constructor=self.yaml_multi_constructors[None ]\n    elif None in self.yaml_constructors:\n     constructor=self.yaml_constructors[None ]\n    elif isinstance(node,ScalarNode):\n     constructor=self.__class__.construct_scalar\n    elif isinstance(node,SequenceNode):\n     constructor=self.__class__.construct_sequence\n    elif isinstance(node,MappingNode):\n     constructor=self.__class__.construct_mapping\n  if tag_suffix is None :\n   data=constructor(self,node)\n  else :\n   data=constructor(self,tag_suffix,node)\n  if isinstance(data,types.GeneratorType):\n   generator=data\n   data=next(generator)\n   if self.deep_construct:\n    for dummy in generator:\n     pass\n   else :\n    self.state_generators.append(generator)\n  self.constructed_objects[node]=data\n  del self.recursive_objects[node]\n  if deep:\n   self.deep_construct=old_deep\n  return data\n  \n def construct_scalar(self,node):\n  if not isinstance(node,ScalarNode):\n   raise ConstructorError(None ,None ,\n   \"expected a scalar node, but found %s\"%node.id,\n   node.start_mark)\n  return node.value\n  \n def construct_sequence(self,node,deep=False ):\n  if not isinstance(node,SequenceNode):\n   raise ConstructorError(None ,None ,\n   \"expected a sequence node, but found %s\"%node.id,\n   node.start_mark)\n  return [self.construct_object(child,deep=deep)\n  for child in node.value]\n  \n def construct_mapping(self,node,deep=False ):\n  if not isinstance(node,MappingNode):\n   raise ConstructorError(None ,None ,\n   \"expected a mapping node, but found %s\"%node.id,\n   node.start_mark)\n  mapping={}\n  for key_node,value_node in node.value:\n   key=self.construct_object(key_node,deep=deep)\n   if not isinstance(key,collections.abc.Hashable):\n    raise ConstructorError(\"while constructing a mapping\",node.start_mark,\n    \"found unhashable key\",key_node.start_mark)\n   value=self.construct_object(value_node,deep=deep)\n   mapping[key]=value\n  return mapping\n  \n def construct_pairs(self,node,deep=False ):\n  if not isinstance(node,MappingNode):\n   raise ConstructorError(None ,None ,\n   \"expected a mapping node, but found %s\"%node.id,\n   node.start_mark)\n  pairs=[]\n  for key_node,value_node in node.value:\n   key=self.construct_object(key_node,deep=deep)\n   value=self.construct_object(value_node,deep=deep)\n   pairs.append((key,value))\n  return pairs\n  \n @classmethod\n def add_constructor(cls,tag,constructor):\n  if not 'yaml_constructors'in cls.__dict__:\n   cls.yaml_constructors=cls.yaml_constructors.copy()\n  cls.yaml_constructors[tag]=constructor\n  \n @classmethod\n def add_multi_constructor(cls,tag_prefix,multi_constructor):\n  if not 'yaml_multi_constructors'in cls.__dict__:\n   cls.yaml_multi_constructors=cls.yaml_multi_constructors.copy()\n  cls.yaml_multi_constructors[tag_prefix]=multi_constructor\n  \nclass SafeConstructor(BaseConstructor):\n\n def construct_scalar(self,node):\n  if isinstance(node,MappingNode):\n   for key_node,value_node in node.value:\n    if key_node.tag =='tag:yaml.org,2002:value':\n     return self.construct_scalar(value_node)\n  return super().construct_scalar(node)\n  \n def flatten_mapping(self,node):\n  merge=[]\n  index=0\n  while index <len(node.value):\n   key_node,value_node=node.value[index]\n   if key_node.tag =='tag:yaml.org,2002:merge':\n    del node.value[index]\n    if isinstance(value_node,MappingNode):\n     self.flatten_mapping(value_node)\n     merge.extend(value_node.value)\n    elif isinstance(value_node,SequenceNode):\n     submerge=[]\n     for subnode in value_node.value:\n      if not isinstance(subnode,MappingNode):\n       raise ConstructorError(\"while constructing a mapping\",\n       node.start_mark,\n       \"expected a mapping for merging, but found %s\"\n       %subnode.id,subnode.start_mark)\n      self.flatten_mapping(subnode)\n      submerge.append(subnode.value)\n     submerge.reverse()\n     for value in submerge:\n      merge.extend(value)\n    else :\n     raise ConstructorError(\"while constructing a mapping\",node.start_mark,\n     \"expected a mapping or list of mappings for merging, but found %s\"\n     %value_node.id,value_node.start_mark)\n   elif key_node.tag =='tag:yaml.org,2002:value':\n    key_node.tag='tag:yaml.org,2002:str'\n    index +=1\n   else :\n    index +=1\n  if merge:\n   node.value=merge+node.value\n   \n def construct_mapping(self,node,deep=False ):\n  if isinstance(node,MappingNode):\n   self.flatten_mapping(node)\n  return super().construct_mapping(node,deep=deep)\n  \n def construct_yaml_null(self,node):\n  self.construct_scalar(node)\n  return None\n  \n bool_values={\n 'yes':True ,\n 'no':False ,\n 'true':True ,\n 'false':False ,\n 'on':True ,\n 'off':False ,\n }\n \n def construct_yaml_bool(self,node):\n  value=self.construct_scalar(node)\n  return self.bool_values[value.lower()]\n  \n def construct_yaml_int(self,node):\n  value=self.construct_scalar(node)\n  value=value.replace('_','')\n  sign=+1\n  if value[0]=='-':\n   sign=-1\n  if value[0]in '+-':\n   value=value[1:]\n  if value =='0':\n   return 0\n  elif value.startswith('0b'):\n   return sign *int(value[2:],2)\n  elif value.startswith('0x'):\n   return sign *int(value[2:],16)\n  elif value[0]=='0':\n   return sign *int(value,8)\n  elif ':'in value:\n   digits=[int(part)for part in value.split(':')]\n   digits.reverse()\n   base=1\n   value=0\n   for digit in digits:\n    value +=digit *base\n    base *=60\n   return sign *value\n  else :\n   return sign *int(value)\n   \n inf_value=1e300\n while inf_value !=inf_value *inf_value:\n  inf_value *=inf_value\n nan_value=-inf_value /inf_value\n \n def construct_yaml_float(self,node):\n  value=self.construct_scalar(node)\n  value=value.replace('_','').lower()\n  sign=+1\n  if value[0]=='-':\n   sign=-1\n  if value[0]in '+-':\n   value=value[1:]\n  if value =='.inf':\n   return sign *self.inf_value\n  elif value =='.nan':\n   return self.nan_value\n  elif ':'in value:\n   digits=[float(part)for part in value.split(':')]\n   digits.reverse()\n   base=1\n   value=0.0\n   for digit in digits:\n    value +=digit *base\n    base *=60\n   return sign *value\n  else :\n   return sign *float(value)\n   \n def construct_yaml_binary(self,node):\n  try :\n   value=self.construct_scalar(node).encode('ascii')\n  except UnicodeEncodeError as exc:\n   raise ConstructorError(None ,None ,\n   \"failed to convert base64 data into ascii: %s\"%exc,\n   node.start_mark)\n  try :\n   if hasattr(base64,'decodebytes'):\n    return base64.decodebytes(value)\n   else :\n    return base64.decodestring(value)\n  except binascii.Error as exc:\n   raise ConstructorError(None ,None ,\n   \"failed to decode base64 data: %s\"%exc,node.start_mark)\n   \n timestamp_regexp=re.compile(\n r'''^(?P<year>[0-9][0-9][0-9][0-9])\n                -(?P<month>[0-9][0-9]?)\n                -(?P<day>[0-9][0-9]?)\n                (?:(?:[Tt]|[ \\t]+)\n                (?P<hour>[0-9][0-9]?)\n                :(?P<minute>[0-9][0-9])\n                :(?P<second>[0-9][0-9])\n                (?:\\.(?P<fraction>[0-9]*))?\n                (?:[ \\t]*(?P<tz>Z|(?P<tz_sign>[-+])(?P<tz_hour>[0-9][0-9]?)\n                (?::(?P<tz_minute>[0-9][0-9]))?))?)?$''',re.X)\n \n def construct_yaml_timestamp(self,node):\n  value=self.construct_scalar(node)\n  match=self.timestamp_regexp.match(node.value)\n  values=match.groupdict()\n  year=int(values['year'])\n  month=int(values['month'])\n  day=int(values['day'])\n  if not values['hour']:\n   return datetime.date(year,month,day)\n  hour=int(values['hour'])\n  minute=int(values['minute'])\n  second=int(values['second'])\n  fraction=0\n  tzinfo=None\n  if values['fraction']:\n   fraction=values['fraction'][:6]\n   while len(fraction)<6:\n    fraction +='0'\n   fraction=int(fraction)\n  if values['tz_sign']:\n   tz_hour=int(values['tz_hour'])\n   tz_minute=int(values['tz_minute']or 0)\n   delta=datetime.timedelta(hours=tz_hour,minutes=tz_minute)\n   if values['tz_sign']=='-':\n    delta=-delta\n   tzinfo=datetime.timezone(delta)\n  elif values['tz']:\n   tzinfo=datetime.timezone.utc\n  return datetime.datetime(year,month,day,hour,minute,second,fraction,\n  tzinfo=tzinfo)\n  \n def construct_yaml_omap(self,node):\n \n \n  omap=[]\n  yield omap\n  if not isinstance(node,SequenceNode):\n   raise ConstructorError(\"while constructing an ordered map\",node.start_mark,\n   \"expected a sequence, but found %s\"%node.id,node.start_mark)\n  for subnode in node.value:\n   if not isinstance(subnode,MappingNode):\n    raise ConstructorError(\"while constructing an ordered map\",node.start_mark,\n    \"expected a mapping of length 1, but found %s\"%subnode.id,\n    subnode.start_mark)\n   if len(subnode.value)!=1:\n    raise ConstructorError(\"while constructing an ordered map\",node.start_mark,\n    \"expected a single mapping item, but found %d items\"%len(subnode.value),\n    subnode.start_mark)\n   key_node,value_node=subnode.value[0]\n   key=self.construct_object(key_node)\n   value=self.construct_object(value_node)\n   omap.append((key,value))\n   \n def construct_yaml_pairs(self,node):\n \n  pairs=[]\n  yield pairs\n  if not isinstance(node,SequenceNode):\n   raise ConstructorError(\"while constructing pairs\",node.start_mark,\n   \"expected a sequence, but found %s\"%node.id,node.start_mark)\n  for subnode in node.value:\n   if not isinstance(subnode,MappingNode):\n    raise ConstructorError(\"while constructing pairs\",node.start_mark,\n    \"expected a mapping of length 1, but found %s\"%subnode.id,\n    subnode.start_mark)\n   if len(subnode.value)!=1:\n    raise ConstructorError(\"while constructing pairs\",node.start_mark,\n    \"expected a single mapping item, but found %d items\"%len(subnode.value),\n    subnode.start_mark)\n   key_node,value_node=subnode.value[0]\n   key=self.construct_object(key_node)\n   value=self.construct_object(value_node)\n   pairs.append((key,value))\n   \n def construct_yaml_set(self,node):\n  data=set()\n  yield data\n  value=self.construct_mapping(node)\n  data.update(value)\n  \n def construct_yaml_str(self,node):\n  return self.construct_scalar(node)\n  \n def construct_yaml_seq(self,node):\n  data=[]\n  yield data\n  data.extend(self.construct_sequence(node))\n  \n def construct_yaml_map(self,node):\n  data={}\n  yield data\n  value=self.construct_mapping(node)\n  data.update(value)\n  \n def construct_yaml_object(self,node,cls):\n  data=cls.__new__(cls)\n  yield data\n  if hasattr(data,'__setstate__'):\n   state=self.construct_mapping(node,deep=True )\n   data.__setstate__(state)\n  else :\n   state=self.construct_mapping(node)\n   data.__dict__.update(state)\n   \n def construct_undefined(self,node):\n  raise ConstructorError(None ,None ,\n  \"could not determine a constructor for the tag %r\"%node.tag,\n  node.start_mark)\n  \nSafeConstructor.add_constructor(\n'tag:yaml.org,2002:null',\nSafeConstructor.construct_yaml_null)\n\nSafeConstructor.add_constructor(\n'tag:yaml.org,2002:bool',\nSafeConstructor.construct_yaml_bool)\n\nSafeConstructor.add_constructor(\n'tag:yaml.org,2002:int',\nSafeConstructor.construct_yaml_int)\n\nSafeConstructor.add_constructor(\n'tag:yaml.org,2002:float',\nSafeConstructor.construct_yaml_float)\n\nSafeConstructor.add_constructor(\n'tag:yaml.org,2002:binary',\nSafeConstructor.construct_yaml_binary)\n\nSafeConstructor.add_constructor(\n'tag:yaml.org,2002:timestamp',\nSafeConstructor.construct_yaml_timestamp)\n\nSafeConstructor.add_constructor(\n'tag:yaml.org,2002:omap',\nSafeConstructor.construct_yaml_omap)\n\nSafeConstructor.add_constructor(\n'tag:yaml.org,2002:pairs',\nSafeConstructor.construct_yaml_pairs)\n\nSafeConstructor.add_constructor(\n'tag:yaml.org,2002:set',\nSafeConstructor.construct_yaml_set)\n\nSafeConstructor.add_constructor(\n'tag:yaml.org,2002:str',\nSafeConstructor.construct_yaml_str)\n\nSafeConstructor.add_constructor(\n'tag:yaml.org,2002:seq',\nSafeConstructor.construct_yaml_seq)\n\nSafeConstructor.add_constructor(\n'tag:yaml.org,2002:map',\nSafeConstructor.construct_yaml_map)\n\nSafeConstructor.add_constructor(None ,\nSafeConstructor.construct_undefined)\n\nclass FullConstructor(SafeConstructor):\n\n\n\n def get_state_keys_blacklist(self):\n  return ['^extend$','^__.*__$']\n  \n def get_state_keys_blacklist_regexp(self):\n  if not hasattr(self,'state_keys_blacklist_regexp'):\n   self.state_keys_blacklist_regexp=re.compile('('+'|'.join(self.get_state_keys_blacklist())+')')\n  return self.state_keys_blacklist_regexp\n  \n def construct_python_str(self,node):\n  return self.construct_scalar(node)\n  \n def construct_python_unicode(self,node):\n  return self.construct_scalar(node)\n  \n def construct_python_bytes(self,node):\n  try :\n   value=self.construct_scalar(node).encode('ascii')\n  except UnicodeEncodeError as exc:\n   raise ConstructorError(None ,None ,\n   \"failed to convert base64 data into ascii: %s\"%exc,\n   node.start_mark)\n  try :\n   if hasattr(base64,'decodebytes'):\n    return base64.decodebytes(value)\n   else :\n    return base64.decodestring(value)\n  except binascii.Error as exc:\n   raise ConstructorError(None ,None ,\n   \"failed to decode base64 data: %s\"%exc,node.start_mark)\n   \n def construct_python_long(self,node):\n  return self.construct_yaml_int(node)\n  \n def construct_python_complex(self,node):\n  return complex(self.construct_scalar(node))\n  \n def construct_python_tuple(self,node):\n  return tuple(self.construct_sequence(node))\n  \n def find_python_module(self,name,mark,unsafe=False ):\n  if not name:\n   raise ConstructorError(\"while constructing a Python module\",mark,\n   \"expected non-empty name appended to the tag\",mark)\n  if unsafe:\n   try :\n    __import__(name)\n   except ImportError as exc:\n    raise ConstructorError(\"while constructing a Python module\",mark,\n    \"cannot find module %r (%s)\"%(name,exc),mark)\n  if name not in sys.modules:\n   raise ConstructorError(\"while constructing a Python module\",mark,\n   \"module %r is not imported\"%name,mark)\n  return sys.modules[name]\n  \n def find_python_name(self,name,mark,unsafe=False ):\n  if not name:\n   raise ConstructorError(\"while constructing a Python object\",mark,\n   \"expected non-empty name appended to the tag\",mark)\n  if '.'in name:\n   module_name,object_name=name.rsplit('.',1)\n  else :\n   module_name='builtins'\n   object_name=name\n  if unsafe:\n   try :\n    __import__(module_name)\n   except ImportError as exc:\n    raise ConstructorError(\"while constructing a Python object\",mark,\n    \"cannot find module %r (%s)\"%(module_name,exc),mark)\n  if module_name not in sys.modules:\n   raise ConstructorError(\"while constructing a Python object\",mark,\n   \"module %r is not imported\"%module_name,mark)\n  module=sys.modules[module_name]\n  if not hasattr(module,object_name):\n   raise ConstructorError(\"while constructing a Python object\",mark,\n   \"cannot find %r in the module %r\"\n   %(object_name,module.__name__),mark)\n  return getattr(module,object_name)\n  \n def construct_python_name(self,suffix,node):\n  value=self.construct_scalar(node)\n  if value:\n   raise ConstructorError(\"while constructing a Python name\",node.start_mark,\n   \"expected the empty value, but found %r\"%value,node.start_mark)\n  return self.find_python_name(suffix,node.start_mark)\n  \n def construct_python_module(self,suffix,node):\n  value=self.construct_scalar(node)\n  if value:\n   raise ConstructorError(\"while constructing a Python module\",node.start_mark,\n   \"expected the empty value, but found %r\"%value,node.start_mark)\n  return self.find_python_module(suffix,node.start_mark)\n  \n def make_python_instance(self,suffix,node,\n args=None ,kwds=None ,newobj=False ,unsafe=False ):\n  if not args:\n   args=[]\n  if not kwds:\n   kwds={}\n  cls=self.find_python_name(suffix,node.start_mark)\n  if not (unsafe or isinstance(cls,type)):\n   raise ConstructorError(\"while constructing a Python instance\",node.start_mark,\n   \"expected a class, but found %r\"%type(cls),\n   node.start_mark)\n  if newobj and isinstance(cls,type):\n   return cls.__new__(cls,*args,**kwds)\n  else :\n   return cls(*args,**kwds)\n   \n def set_python_instance_state(self,instance,state,unsafe=False ):\n  if hasattr(instance,'__setstate__'):\n   instance.__setstate__(state)\n  else :\n   slotstate={}\n   if isinstance(state,tuple)and len(state)==2:\n    state,slotstate=state\n   if hasattr(instance,'__dict__'):\n    if not unsafe and state:\n     for key in state.keys():\n      self.check_state_key(key)\n    instance.__dict__.update(state)\n   elif state:\n    slotstate.update(state)\n   for key,value in slotstate.items():\n    if not unsafe:\n     self.check_state_key(key)\n    setattr(instance,key,value)\n    \n def construct_python_object(self,suffix,node):\n \n \n  instance=self.make_python_instance(suffix,node,newobj=True )\n  yield instance\n  deep=hasattr(instance,'__setstate__')\n  state=self.construct_mapping(node,deep=deep)\n  self.set_python_instance_state(instance,state)\n  \n def construct_python_object_apply(self,suffix,node,newobj=False ):\n \n \n \n \n \n \n \n \n \n \n \n  if isinstance(node,SequenceNode):\n   args=self.construct_sequence(node,deep=True )\n   kwds={}\n   state={}\n   listitems=[]\n   dictitems={}\n  else :\n   value=self.construct_mapping(node,deep=True )\n   args=value.get('args',[])\n   kwds=value.get('kwds',{})\n   state=value.get('state',{})\n   listitems=value.get('listitems',[])\n   dictitems=value.get('dictitems',{})\n  instance=self.make_python_instance(suffix,node,args,kwds,newobj)\n  if state:\n   self.set_python_instance_state(instance,state)\n  if listitems:\n   instance.extend(listitems)\n  if dictitems:\n   for key in dictitems:\n    instance[key]=dictitems[key]\n  return instance\n  \n def construct_python_object_new(self,suffix,node):\n  return self.construct_python_object_apply(suffix,node,newobj=True )\n  \nFullConstructor.add_constructor(\n'tag:yaml.org,2002:python/none',\nFullConstructor.construct_yaml_null)\n\nFullConstructor.add_constructor(\n'tag:yaml.org,2002:python/bool',\nFullConstructor.construct_yaml_bool)\n\nFullConstructor.add_constructor(\n'tag:yaml.org,2002:python/str',\nFullConstructor.construct_python_str)\n\nFullConstructor.add_constructor(\n'tag:yaml.org,2002:python/unicode',\nFullConstructor.construct_python_unicode)\n\nFullConstructor.add_constructor(\n'tag:yaml.org,2002:python/bytes',\nFullConstructor.construct_python_bytes)\n\nFullConstructor.add_constructor(\n'tag:yaml.org,2002:python/int',\nFullConstructor.construct_yaml_int)\n\nFullConstructor.add_constructor(\n'tag:yaml.org,2002:python/long',\nFullConstructor.construct_python_long)\n\nFullConstructor.add_constructor(\n'tag:yaml.org,2002:python/float',\nFullConstructor.construct_yaml_float)\n\nFullConstructor.add_constructor(\n'tag:yaml.org,2002:python/complex',\nFullConstructor.construct_python_complex)\n\nFullConstructor.add_constructor(\n'tag:yaml.org,2002:python/list',\nFullConstructor.construct_yaml_seq)\n\nFullConstructor.add_constructor(\n'tag:yaml.org,2002:python/tuple',\nFullConstructor.construct_python_tuple)\n\nFullConstructor.add_constructor(\n'tag:yaml.org,2002:python/dict',\nFullConstructor.construct_yaml_map)\n\nFullConstructor.add_multi_constructor(\n'tag:yaml.org,2002:python/name:',\nFullConstructor.construct_python_name)\n\nclass UnsafeConstructor(FullConstructor):\n\n def find_python_module(self,name,mark):\n  return super(UnsafeConstructor,self).find_python_module(name,mark,unsafe=True )\n  \n def find_python_name(self,name,mark):\n  return super(UnsafeConstructor,self).find_python_name(name,mark,unsafe=True )\n  \n def make_python_instance(self,suffix,node,args=None ,kwds=None ,newobj=False ):\n  return super(UnsafeConstructor,self).make_python_instance(\n  suffix,node,args,kwds,newobj,unsafe=True )\n  \n def set_python_instance_state(self,instance,state):\n  return super(UnsafeConstructor,self).set_python_instance_state(\n  instance,state,unsafe=True )\n  \nUnsafeConstructor.add_multi_constructor(\n'tag:yaml.org,2002:python/module:',\nUnsafeConstructor.construct_python_module)\n\nUnsafeConstructor.add_multi_constructor(\n'tag:yaml.org,2002:python/object:',\nUnsafeConstructor.construct_python_object)\n\nUnsafeConstructor.add_multi_constructor(\n'tag:yaml.org,2002:python/object/new:',\nUnsafeConstructor.construct_python_object_new)\n\nUnsafeConstructor.add_multi_constructor(\n'tag:yaml.org,2002:python/object/apply:',\nUnsafeConstructor.construct_python_object_apply)\n\n\n\nclass Constructor(UnsafeConstructor):\n pass\n", ["base64", "binascii", "collections.abc", "datetime", "re", "sys", "types", "yaml.error", "yaml.nodes"]], "yaml.composer": [".py", "\n__all__=['Composer','ComposerError']\n\nfrom .error import MarkedYAMLError\nfrom .events import *\nfrom .nodes import *\n\nclass ComposerError(MarkedYAMLError):\n pass\n \nclass Composer:\n\n def __init__(self):\n  self.anchors={}\n  \n def check_node(self):\n \n  if self.check_event(StreamStartEvent):\n   self.get_event()\n   \n   \n  return not self.check_event(StreamEndEvent)\n  \n def get_node(self):\n \n  if not self.check_event(StreamEndEvent):\n   return self.compose_document()\n   \n def get_single_node(self):\n \n  self.get_event()\n  \n  \n  document=None\n  if not self.check_event(StreamEndEvent):\n   document=self.compose_document()\n   \n   \n  if not self.check_event(StreamEndEvent):\n   event=self.get_event()\n   raise ComposerError(\"expected a single document in the stream\",\n   document.start_mark,\"but found another document\",\n   event.start_mark)\n   \n   \n  self.get_event()\n  \n  return document\n  \n def compose_document(self):\n \n  self.get_event()\n  \n  \n  node=self.compose_node(None ,None )\n  \n  \n  self.get_event()\n  \n  self.anchors={}\n  return node\n  \n def compose_node(self,parent,index):\n  if self.check_event(AliasEvent):\n   event=self.get_event()\n   anchor=event.anchor\n   if anchor not in self.anchors:\n    raise ComposerError(None ,None ,\"found undefined alias %r\"\n    %anchor,event.start_mark)\n   return self.anchors[anchor]\n  event=self.peek_event()\n  anchor=event.anchor\n  if anchor is not None :\n   if anchor in self.anchors:\n    raise ComposerError(\"found duplicate anchor %r; first occurrence\"\n    %anchor,self.anchors[anchor].start_mark,\n    \"second occurrence\",event.start_mark)\n  self.descend_resolver(parent,index)\n  if self.check_event(ScalarEvent):\n   node=self.compose_scalar_node(anchor)\n  elif self.check_event(SequenceStartEvent):\n   node=self.compose_sequence_node(anchor)\n  elif self.check_event(MappingStartEvent):\n   node=self.compose_mapping_node(anchor)\n  self.ascend_resolver()\n  return node\n  \n def compose_scalar_node(self,anchor):\n  event=self.get_event()\n  tag=event.tag\n  if tag is None or tag =='!':\n   tag=self.resolve(ScalarNode,event.value,event.implicit)\n  node=ScalarNode(tag,event.value,\n  event.start_mark,event.end_mark,style=event.style)\n  if anchor is not None :\n   self.anchors[anchor]=node\n  return node\n  \n def compose_sequence_node(self,anchor):\n  start_event=self.get_event()\n  tag=start_event.tag\n  if tag is None or tag =='!':\n   tag=self.resolve(SequenceNode,None ,start_event.implicit)\n  node=SequenceNode(tag,[],\n  start_event.start_mark,None ,\n  flow_style=start_event.flow_style)\n  if anchor is not None :\n   self.anchors[anchor]=node\n  index=0\n  while not self.check_event(SequenceEndEvent):\n   node.value.append(self.compose_node(node,index))\n   index +=1\n  end_event=self.get_event()\n  node.end_mark=end_event.end_mark\n  return node\n  \n def compose_mapping_node(self,anchor):\n  start_event=self.get_event()\n  tag=start_event.tag\n  if tag is None or tag =='!':\n   tag=self.resolve(MappingNode,None ,start_event.implicit)\n  node=MappingNode(tag,[],\n  start_event.start_mark,None ,\n  flow_style=start_event.flow_style)\n  if anchor is not None :\n   self.anchors[anchor]=node\n  while not self.check_event(MappingEndEvent):\n  \n   item_key=self.compose_node(node,None )\n   \n   \n   \n   item_value=self.compose_node(node,item_key)\n   \n   node.value.append((item_key,item_value))\n  end_event=self.get_event()\n  node.end_mark=end_event.end_mark\n  return node\n  \n", ["yaml.error", "yaml.events", "yaml.nodes"]], "yaml.events": [".py", "\n\n\nclass Event(object):\n def __init__(self,start_mark=None ,end_mark=None ):\n  self.start_mark=start_mark\n  self.end_mark=end_mark\n def __repr__(self):\n  attributes=[key for key in ['anchor','tag','implicit','value']\n  if hasattr(self,key)]\n  arguments=', '.join(['%s=%r'%(key,getattr(self,key))\n  for key in attributes])\n  return '%s(%s)'%(self.__class__.__name__,arguments)\n  \nclass NodeEvent(Event):\n def __init__(self,anchor,start_mark=None ,end_mark=None ):\n  self.anchor=anchor\n  self.start_mark=start_mark\n  self.end_mark=end_mark\n  \nclass CollectionStartEvent(NodeEvent):\n def __init__(self,anchor,tag,implicit,start_mark=None ,end_mark=None ,\n flow_style=None ):\n  self.anchor=anchor\n  self.tag=tag\n  self.implicit=implicit\n  self.start_mark=start_mark\n  self.end_mark=end_mark\n  self.flow_style=flow_style\n  \nclass CollectionEndEvent(Event):\n pass\n \n \n \nclass StreamStartEvent(Event):\n def __init__(self,start_mark=None ,end_mark=None ,encoding=None ):\n  self.start_mark=start_mark\n  self.end_mark=end_mark\n  self.encoding=encoding\n  \nclass StreamEndEvent(Event):\n pass\n \nclass DocumentStartEvent(Event):\n def __init__(self,start_mark=None ,end_mark=None ,\n explicit=None ,version=None ,tags=None ):\n  self.start_mark=start_mark\n  self.end_mark=end_mark\n  self.explicit=explicit\n  self.version=version\n  self.tags=tags\n  \nclass DocumentEndEvent(Event):\n def __init__(self,start_mark=None ,end_mark=None ,\n explicit=None ):\n  self.start_mark=start_mark\n  self.end_mark=end_mark\n  self.explicit=explicit\n  \nclass AliasEvent(NodeEvent):\n pass\n \nclass ScalarEvent(NodeEvent):\n def __init__(self,anchor,tag,implicit,value,\n start_mark=None ,end_mark=None ,style=None ):\n  self.anchor=anchor\n  self.tag=tag\n  self.implicit=implicit\n  self.value=value\n  self.start_mark=start_mark\n  self.end_mark=end_mark\n  self.style=style\n  \nclass SequenceStartEvent(CollectionStartEvent):\n pass\n \nclass SequenceEndEvent(CollectionEndEvent):\n pass\n \nclass MappingStartEvent(CollectionStartEvent):\n pass\n \nclass MappingEndEvent(CollectionEndEvent):\n pass\n \n", []], "yaml": [".py", "\nfrom .error import *\n\nfrom .tokens import *\nfrom .events import *\nfrom .nodes import *\n\nfrom .loader import *\nfrom .dumper import *\n\n__version__='6.0.1'\ntry :\n from .cyaml import *\n __with_libyaml__=True\nexcept ImportError:\n __with_libyaml__=False\n \nimport io\n\n\n\n\n\ndef warnings(settings=None ):\n if settings is None :\n  return {}\n  \n  \ndef scan(stream,Loader=Loader):\n ''\n\n \n loader=Loader(stream)\n try :\n  while loader.check_token():\n   yield loader.get_token()\n finally :\n  loader.dispose()\n  \ndef parse(stream,Loader=Loader):\n ''\n\n \n loader=Loader(stream)\n try :\n  while loader.check_event():\n   yield loader.get_event()\n finally :\n  loader.dispose()\n  \ndef compose(stream,Loader=Loader):\n ''\n\n\n \n loader=Loader(stream)\n try :\n  return loader.get_single_node()\n finally :\n  loader.dispose()\n  \ndef compose_all(stream,Loader=Loader):\n ''\n\n\n \n loader=Loader(stream)\n try :\n  while loader.check_node():\n   yield loader.get_node()\n finally :\n  loader.dispose()\n  \ndef load(stream,Loader):\n ''\n\n\n \n loader=Loader(stream)\n try :\n  return loader.get_single_data()\n finally :\n  loader.dispose()\n  \ndef load_all(stream,Loader):\n ''\n\n\n \n loader=Loader(stream)\n try :\n  while loader.check_data():\n   yield loader.get_data()\n finally :\n  loader.dispose()\n  \ndef full_load(stream):\n ''\n\n\n\n\n\n \n return load(stream,FullLoader)\n \ndef full_load_all(stream):\n ''\n\n\n\n\n\n \n return load_all(stream,FullLoader)\n \ndef safe_load(stream):\n ''\n\n\n\n\n\n \n return load(stream,SafeLoader)\n \ndef safe_load_all(stream):\n ''\n\n\n\n\n\n \n return load_all(stream,SafeLoader)\n \ndef unsafe_load(stream):\n ''\n\n\n\n\n\n \n return load(stream,UnsafeLoader)\n \ndef unsafe_load_all(stream):\n ''\n\n\n\n\n\n \n return load_all(stream,UnsafeLoader)\n \ndef emit(events,stream=None ,Dumper=Dumper,\ncanonical=None ,indent=None ,width=None ,\nallow_unicode=None ,line_break=None ):\n ''\n\n\n \n getvalue=None\n if stream is None :\n  stream=io.StringIO()\n  getvalue=stream.getvalue\n dumper=Dumper(stream,canonical=canonical,indent=indent,width=width,\n allow_unicode=allow_unicode,line_break=line_break)\n try :\n  for event in events:\n   dumper.emit(event)\n finally :\n  dumper.dispose()\n if getvalue:\n  return getvalue()\n  \ndef serialize_all(nodes,stream=None ,Dumper=Dumper,\ncanonical=None ,indent=None ,width=None ,\nallow_unicode=None ,line_break=None ,\nencoding=None ,explicit_start=None ,explicit_end=None ,\nversion=None ,tags=None ):\n ''\n\n\n \n getvalue=None\n if stream is None :\n  if encoding is None :\n   stream=io.StringIO()\n  else :\n   stream=io.BytesIO()\n  getvalue=stream.getvalue\n dumper=Dumper(stream,canonical=canonical,indent=indent,width=width,\n allow_unicode=allow_unicode,line_break=line_break,\n encoding=encoding,version=version,tags=tags,\n explicit_start=explicit_start,explicit_end=explicit_end)\n try :\n  dumper.open()\n  for node in nodes:\n   dumper.serialize(node)\n  dumper.close()\n finally :\n  dumper.dispose()\n if getvalue:\n  return getvalue()\n  \ndef serialize(node,stream=None ,Dumper=Dumper,**kwds):\n ''\n\n\n \n return serialize_all([node],stream,Dumper=Dumper,**kwds)\n \ndef dump_all(documents,stream=None ,Dumper=Dumper,\ndefault_style=None ,default_flow_style=False ,\ncanonical=None ,indent=None ,width=None ,\nallow_unicode=None ,line_break=None ,\nencoding=None ,explicit_start=None ,explicit_end=None ,\nversion=None ,tags=None ,sort_keys=True ):\n ''\n\n\n \n getvalue=None\n if stream is None :\n  if encoding is None :\n   stream=io.StringIO()\n  else :\n   stream=io.BytesIO()\n  getvalue=stream.getvalue\n dumper=Dumper(stream,default_style=default_style,\n default_flow_style=default_flow_style,\n canonical=canonical,indent=indent,width=width,\n allow_unicode=allow_unicode,line_break=line_break,\n encoding=encoding,version=version,tags=tags,\n explicit_start=explicit_start,explicit_end=explicit_end,sort_keys=sort_keys)\n try :\n  dumper.open()\n  for data in documents:\n   dumper.represent(data)\n  dumper.close()\n finally :\n  dumper.dispose()\n if getvalue:\n  return getvalue()\n  \ndef dump(data,stream=None ,Dumper=Dumper,**kwds):\n ''\n\n\n \n return dump_all([data],stream,Dumper=Dumper,**kwds)\n \ndef safe_dump_all(documents,stream=None ,**kwds):\n ''\n\n\n\n \n return dump_all(documents,stream,Dumper=SafeDumper,**kwds)\n \ndef safe_dump(data,stream=None ,**kwds):\n ''\n\n\n\n \n return dump_all([data],stream,Dumper=SafeDumper,**kwds)\n \ndef add_implicit_resolver(tag,regexp,first=None ,\nLoader=None ,Dumper=Dumper):\n ''\n\n\n\n\n \n if Loader is None :\n  loader.Loader.add_implicit_resolver(tag,regexp,first)\n  loader.FullLoader.add_implicit_resolver(tag,regexp,first)\n  loader.UnsafeLoader.add_implicit_resolver(tag,regexp,first)\n else :\n  Loader.add_implicit_resolver(tag,regexp,first)\n Dumper.add_implicit_resolver(tag,regexp,first)\n \ndef add_path_resolver(tag,path,kind=None ,Loader=None ,Dumper=Dumper):\n ''\n\n\n\n\n \n if Loader is None :\n  loader.Loader.add_path_resolver(tag,path,kind)\n  loader.FullLoader.add_path_resolver(tag,path,kind)\n  loader.UnsafeLoader.add_path_resolver(tag,path,kind)\n else :\n  Loader.add_path_resolver(tag,path,kind)\n Dumper.add_path_resolver(tag,path,kind)\n \ndef add_constructor(tag,constructor,Loader=None ):\n ''\n\n\n\n \n if Loader is None :\n  loader.Loader.add_constructor(tag,constructor)\n  loader.FullLoader.add_constructor(tag,constructor)\n  loader.UnsafeLoader.add_constructor(tag,constructor)\n else :\n  Loader.add_constructor(tag,constructor)\n  \ndef add_multi_constructor(tag_prefix,multi_constructor,Loader=None ):\n ''\n\n\n\n\n \n if Loader is None :\n  loader.Loader.add_multi_constructor(tag_prefix,multi_constructor)\n  loader.FullLoader.add_multi_constructor(tag_prefix,multi_constructor)\n  loader.UnsafeLoader.add_multi_constructor(tag_prefix,multi_constructor)\n else :\n  Loader.add_multi_constructor(tag_prefix,multi_constructor)\n  \ndef add_representer(data_type,representer,Dumper=Dumper):\n ''\n\n\n\n\n \n Dumper.add_representer(data_type,representer)\n \ndef add_multi_representer(data_type,multi_representer,Dumper=Dumper):\n ''\n\n\n\n\n \n Dumper.add_multi_representer(data_type,multi_representer)\n \nclass YAMLObjectMetaclass(type):\n ''\n\n \n def __init__(cls,name,bases,kwds):\n  super(YAMLObjectMetaclass,cls).__init__(name,bases,kwds)\n  if 'yaml_tag'in kwds and kwds['yaml_tag']is not None :\n   if isinstance(cls.yaml_loader,list):\n    for loader in cls.yaml_loader:\n     loader.add_constructor(cls.yaml_tag,cls.from_yaml)\n   else :\n    cls.yaml_loader.add_constructor(cls.yaml_tag,cls.from_yaml)\n    \n   cls.yaml_dumper.add_representer(cls,cls.to_yaml)\n   \nclass YAMLObject(metaclass=YAMLObjectMetaclass):\n ''\n\n\n \n \n __slots__=()\n \n yaml_loader=[Loader,FullLoader,UnsafeLoader]\n yaml_dumper=Dumper\n \n yaml_tag=None\n yaml_flow_style=None\n \n @classmethod\n def from_yaml(cls,loader,node):\n  ''\n\n  \n  return loader.construct_yaml_object(node,cls)\n  \n @classmethod\n def to_yaml(cls,dumper,data):\n  ''\n\n  \n  return dumper.represent_yaml_object(cls.yaml_tag,data,cls,\n  flow_style=cls.yaml_flow_style)\n  \n", ["io", "yaml.cyaml", "yaml.dumper", "yaml.error", "yaml.events", "yaml.loader", "yaml.nodes", "yaml.tokens"], 1], "yaml.representer": [".py", "\n__all__=['BaseRepresenter','SafeRepresenter','Representer',\n'RepresenterError']\n\nfrom .error import *\nfrom .nodes import *\n\nimport datetime,copyreg,types,base64,collections\n\nclass RepresenterError(YAMLError):\n pass\n \nclass BaseRepresenter:\n\n yaml_representers={}\n yaml_multi_representers={}\n \n def __init__(self,default_style=None ,default_flow_style=False ,sort_keys=True ):\n  self.default_style=default_style\n  self.sort_keys=sort_keys\n  self.default_flow_style=default_flow_style\n  self.represented_objects={}\n  self.object_keeper=[]\n  self.alias_key=None\n  \n def represent(self,data):\n  node=self.represent_data(data)\n  self.serialize(node)\n  self.represented_objects={}\n  self.object_keeper=[]\n  self.alias_key=None\n  \n def represent_data(self,data):\n  if self.ignore_aliases(data):\n   self.alias_key=None\n  else :\n   self.alias_key=id(data)\n  if self.alias_key is not None :\n   if self.alias_key in self.represented_objects:\n    node=self.represented_objects[self.alias_key]\n    \n    \n    return node\n    \n   self.object_keeper.append(data)\n  data_types=type(data).__mro__\n  if data_types[0]in self.yaml_representers:\n   node=self.yaml_representers[data_types[0]](self,data)\n  else :\n   for data_type in data_types:\n    if data_type in self.yaml_multi_representers:\n     node=self.yaml_multi_representers[data_type](self,data)\n     break\n   else :\n    if None in self.yaml_multi_representers:\n     node=self.yaml_multi_representers[None ](self,data)\n    elif None in self.yaml_representers:\n     node=self.yaml_representers[None ](self,data)\n    else :\n     node=ScalarNode(None ,str(data))\n     \n     \n  return node\n  \n @classmethod\n def add_representer(cls,data_type,representer):\n  if not 'yaml_representers'in cls.__dict__:\n   cls.yaml_representers=cls.yaml_representers.copy()\n  cls.yaml_representers[data_type]=representer\n  \n @classmethod\n def add_multi_representer(cls,data_type,representer):\n  if not 'yaml_multi_representers'in cls.__dict__:\n   cls.yaml_multi_representers=cls.yaml_multi_representers.copy()\n  cls.yaml_multi_representers[data_type]=representer\n  \n def represent_scalar(self,tag,value,style=None ):\n  if style is None :\n   style=self.default_style\n  node=ScalarNode(tag,value,style=style)\n  if self.alias_key is not None :\n   self.represented_objects[self.alias_key]=node\n  return node\n  \n def represent_sequence(self,tag,sequence,flow_style=None ):\n  value=[]\n  node=SequenceNode(tag,value,flow_style=flow_style)\n  if self.alias_key is not None :\n   self.represented_objects[self.alias_key]=node\n  best_style=True\n  for item in sequence:\n   node_item=self.represent_data(item)\n   if not (isinstance(node_item,ScalarNode)and not node_item.style):\n    best_style=False\n   value.append(node_item)\n  if flow_style is None :\n   if self.default_flow_style is not None :\n    node.flow_style=self.default_flow_style\n   else :\n    node.flow_style=best_style\n  return node\n  \n def represent_mapping(self,tag,mapping,flow_style=None ):\n  value=[]\n  node=MappingNode(tag,value,flow_style=flow_style)\n  if self.alias_key is not None :\n   self.represented_objects[self.alias_key]=node\n  best_style=True\n  if hasattr(mapping,'items'):\n   mapping=list(mapping.items())\n   if self.sort_keys:\n    try :\n     mapping=sorted(mapping)\n    except TypeError:\n     pass\n  for item_key,item_value in mapping:\n   node_key=self.represent_data(item_key)\n   node_value=self.represent_data(item_value)\n   if not (isinstance(node_key,ScalarNode)and not node_key.style):\n    best_style=False\n   if not (isinstance(node_value,ScalarNode)and not node_value.style):\n    best_style=False\n   value.append((node_key,node_value))\n  if flow_style is None :\n   if self.default_flow_style is not None :\n    node.flow_style=self.default_flow_style\n   else :\n    node.flow_style=best_style\n  return node\n  \n def ignore_aliases(self,data):\n  return False\n  \nclass SafeRepresenter(BaseRepresenter):\n\n def ignore_aliases(self,data):\n  if data is None :\n   return True\n  if isinstance(data,tuple)and data ==():\n   return True\n  if isinstance(data,(str,bytes,bool,int,float)):\n   return True\n   \n def represent_none(self,data):\n  return self.represent_scalar('tag:yaml.org,2002:null','null')\n  \n def represent_str(self,data):\n  return self.represent_scalar('tag:yaml.org,2002:str',data)\n  \n def represent_binary(self,data):\n  if hasattr(base64,'encodebytes'):\n   data=base64.encodebytes(data).decode('ascii')\n  else :\n   data=base64.encodestring(data).decode('ascii')\n  return self.represent_scalar('tag:yaml.org,2002:binary',data,style='|')\n  \n def represent_bool(self,data):\n  if data:\n   value='true'\n  else :\n   value='false'\n  return self.represent_scalar('tag:yaml.org,2002:bool',value)\n  \n def represent_int(self,data):\n  return self.represent_scalar('tag:yaml.org,2002:int',str(data))\n  \n inf_value=1e300\n while repr(inf_value)!=repr(inf_value *inf_value):\n  inf_value *=inf_value\n  \n def represent_float(self,data):\n  if data !=data or (data ==0.0 and data ==1.0):\n   value='.nan'\n  elif data ==self.inf_value:\n   value='.inf'\n  elif data ==-self.inf_value:\n   value='-.inf'\n  else :\n   value=repr(data).lower()\n   \n   \n   \n   \n   \n   \n   \n   if '.'not in value and 'e'in value:\n    value=value.replace('e','.0e',1)\n  return self.represent_scalar('tag:yaml.org,2002:float',value)\n  \n def represent_list(self,data):\n \n \n \n \n \n \n \n  return self.represent_sequence('tag:yaml.org,2002:seq',data)\n  \n  \n  \n  \n  \n  \n def represent_dict(self,data):\n  return self.represent_mapping('tag:yaml.org,2002:map',data)\n  \n def represent_set(self,data):\n  value={}\n  for key in data:\n   value[key]=None\n  return self.represent_mapping('tag:yaml.org,2002:set',value)\n  \n def represent_date(self,data):\n  value=data.isoformat()\n  return self.represent_scalar('tag:yaml.org,2002:timestamp',value)\n  \n def represent_datetime(self,data):\n  value=data.isoformat(' ')\n  return self.represent_scalar('tag:yaml.org,2002:timestamp',value)\n  \n def represent_yaml_object(self,tag,data,cls,flow_style=None ):\n  if hasattr(data,'__getstate__'):\n   state=data.__getstate__()\n  else :\n   state=data.__dict__.copy()\n  return self.represent_mapping(tag,state,flow_style=flow_style)\n  \n def represent_undefined(self,data):\n  raise RepresenterError(\"cannot represent an object\",data)\n  \nSafeRepresenter.add_representer(type(None ),\nSafeRepresenter.represent_none)\n\nSafeRepresenter.add_representer(str,\nSafeRepresenter.represent_str)\n\nSafeRepresenter.add_representer(bytes,\nSafeRepresenter.represent_binary)\n\nSafeRepresenter.add_representer(bool,\nSafeRepresenter.represent_bool)\n\nSafeRepresenter.add_representer(int,\nSafeRepresenter.represent_int)\n\nSafeRepresenter.add_representer(float,\nSafeRepresenter.represent_float)\n\nSafeRepresenter.add_representer(list,\nSafeRepresenter.represent_list)\n\nSafeRepresenter.add_representer(tuple,\nSafeRepresenter.represent_list)\n\nSafeRepresenter.add_representer(dict,\nSafeRepresenter.represent_dict)\n\nSafeRepresenter.add_representer(set,\nSafeRepresenter.represent_set)\n\nSafeRepresenter.add_representer(datetime.date,\nSafeRepresenter.represent_date)\n\nSafeRepresenter.add_representer(datetime.datetime,\nSafeRepresenter.represent_datetime)\n\nSafeRepresenter.add_representer(None ,\nSafeRepresenter.represent_undefined)\n\nclass Representer(SafeRepresenter):\n\n def represent_complex(self,data):\n  if data.imag ==0.0:\n   data='%r'%data.real\n  elif data.real ==0.0:\n   data='%rj'%data.imag\n  elif data.imag >0:\n   data='%r+%rj'%(data.real,data.imag)\n  else :\n   data='%r%rj'%(data.real,data.imag)\n  return self.represent_scalar('tag:yaml.org,2002:python/complex',data)\n  \n def represent_tuple(self,data):\n  return self.represent_sequence('tag:yaml.org,2002:python/tuple',data)\n  \n def represent_name(self,data):\n  name='%s.%s'%(data.__module__,data.__name__)\n  return self.represent_scalar('tag:yaml.org,2002:python/name:'+name,'')\n  \n def represent_module(self,data):\n  return self.represent_scalar(\n  'tag:yaml.org,2002:python/module:'+data.__name__,'')\n  \n def represent_object(self,data):\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n  cls=type(data)\n  if cls in copyreg.dispatch_table:\n   reduce=copyreg.dispatch_table[cls](data)\n  elif hasattr(data,'__reduce_ex__'):\n   reduce=data.__reduce_ex__(2)\n  elif hasattr(data,'__reduce__'):\n   reduce=data.__reduce__()\n  else :\n   raise RepresenterError(\"cannot represent an object\",data)\n  reduce=(list(reduce)+[None ]*5)[:5]\n  function,args,state,listitems,dictitems=reduce\n  args=list(args)\n  if state is None :\n   state={}\n  if listitems is not None :\n   listitems=list(listitems)\n  if dictitems is not None :\n   dictitems=dict(dictitems)\n  if function.__name__ =='__newobj__':\n   function=args[0]\n   args=args[1:]\n   tag='tag:yaml.org,2002:python/object/new:'\n   newobj=True\n  else :\n   tag='tag:yaml.org,2002:python/object/apply:'\n   newobj=False\n  function_name='%s.%s'%(function.__module__,function.__name__)\n  if not args and not listitems and not dictitems\\\n  and isinstance(state,dict)and newobj:\n   return self.represent_mapping(\n   'tag:yaml.org,2002:python/object:'+function_name,state)\n  if not listitems and not dictitems\\\n  and isinstance(state,dict)and not state:\n   return self.represent_sequence(tag+function_name,args)\n  value={}\n  if args:\n   value['args']=args\n  if state or not isinstance(state,dict):\n   value['state']=state\n  if listitems:\n   value['listitems']=listitems\n  if dictitems:\n   value['dictitems']=dictitems\n  return self.represent_mapping(tag+function_name,value)\n  \n def represent_ordered_dict(self,data):\n \n  data_type=type(data)\n  tag='tag:yaml.org,2002:python/object/apply:%s.%s'\\\n  %(data_type.__module__,data_type.__name__)\n  items=[[key,value]for key,value in data.items()]\n  return self.represent_sequence(tag,[items])\n  \nRepresenter.add_representer(complex,\nRepresenter.represent_complex)\n\nRepresenter.add_representer(tuple,\nRepresenter.represent_tuple)\n\nRepresenter.add_multi_representer(type,\nRepresenter.represent_name)\n\nRepresenter.add_representer(collections.OrderedDict,\nRepresenter.represent_ordered_dict)\n\nRepresenter.add_representer(types.FunctionType,\nRepresenter.represent_name)\n\nRepresenter.add_representer(types.BuiltinFunctionType,\nRepresenter.represent_name)\n\nRepresenter.add_representer(types.ModuleType,\nRepresenter.represent_module)\n\nRepresenter.add_multi_representer(object,\nRepresenter.represent_object)\n\n", ["base64", "collections", "copyreg", "datetime", "types", "yaml.error", "yaml.nodes"]], "yaml.tokens": [".py", "\nclass Token(object):\n def __init__(self,start_mark,end_mark):\n  self.start_mark=start_mark\n  self.end_mark=end_mark\n def __repr__(self):\n  attributes=[key for key in self.__dict__\n  if not key.endswith('_mark')]\n  attributes.sort()\n  arguments=', '.join(['%s=%r'%(key,getattr(self,key))\n  for key in attributes])\n  return '%s(%s)'%(self.__class__.__name__,arguments)\n  \n  \n  \n  \nclass DirectiveToken(Token):\n id='<directive>'\n def __init__(self,name,value,start_mark,end_mark):\n  self.name=name\n  self.value=value\n  self.start_mark=start_mark\n  self.end_mark=end_mark\n  \nclass DocumentStartToken(Token):\n id='<document start>'\n \nclass DocumentEndToken(Token):\n id='<document end>'\n \nclass StreamStartToken(Token):\n id='<stream start>'\n def __init__(self,start_mark=None ,end_mark=None ,\n encoding=None ):\n  self.start_mark=start_mark\n  self.end_mark=end_mark\n  self.encoding=encoding\n  \nclass StreamEndToken(Token):\n id='<stream end>'\n \nclass BlockSequenceStartToken(Token):\n id='<block sequence start>'\n \nclass BlockMappingStartToken(Token):\n id='<block mapping start>'\n \nclass BlockEndToken(Token):\n id='<block end>'\n \nclass FlowSequenceStartToken(Token):\n id='['\n \nclass FlowMappingStartToken(Token):\n id='{'\n \nclass FlowSequenceEndToken(Token):\n id=']'\n \nclass FlowMappingEndToken(Token):\n id='}'\n \nclass KeyToken(Token):\n id='?'\n \nclass ValueToken(Token):\n id=':'\n \nclass BlockEntryToken(Token):\n id='-'\n \nclass FlowEntryToken(Token):\n id=','\n \nclass AliasToken(Token):\n id='<alias>'\n def __init__(self,value,start_mark,end_mark):\n  self.value=value\n  self.start_mark=start_mark\n  self.end_mark=end_mark\n  \nclass AnchorToken(Token):\n id='<anchor>'\n def __init__(self,value,start_mark,end_mark):\n  self.value=value\n  self.start_mark=start_mark\n  self.end_mark=end_mark\n  \nclass TagToken(Token):\n id='<tag>'\n def __init__(self,value,start_mark,end_mark):\n  self.value=value\n  self.start_mark=start_mark\n  self.end_mark=end_mark\n  \nclass ScalarToken(Token):\n id='<scalar>'\n def __init__(self,value,plain,start_mark,end_mark,style=None ):\n  self.value=value\n  self.plain=plain\n  self.start_mark=start_mark\n  self.end_mark=end_mark\n  self.style=style\n  \n", []], "yaml.dumper": [".py", "\n__all__=['BaseDumper','SafeDumper','Dumper']\n\nfrom .emitter import *\nfrom .serializer import *\nfrom .representer import *\nfrom .resolver import *\n\nclass BaseDumper(Emitter,Serializer,BaseRepresenter,BaseResolver):\n\n def __init__(self,stream,\n default_style=None ,default_flow_style=False ,\n canonical=None ,indent=None ,width=None ,\n allow_unicode=None ,line_break=None ,\n encoding=None ,explicit_start=None ,explicit_end=None ,\n version=None ,tags=None ,sort_keys=True ):\n  Emitter.__init__(self,stream,canonical=canonical,\n  indent=indent,width=width,\n  allow_unicode=allow_unicode,line_break=line_break)\n  Serializer.__init__(self,encoding=encoding,\n  explicit_start=explicit_start,explicit_end=explicit_end,\n  version=version,tags=tags)\n  Representer.__init__(self,default_style=default_style,\n  default_flow_style=default_flow_style,sort_keys=sort_keys)\n  Resolver.__init__(self)\n  \nclass SafeDumper(Emitter,Serializer,SafeRepresenter,Resolver):\n\n def __init__(self,stream,\n default_style=None ,default_flow_style=False ,\n canonical=None ,indent=None ,width=None ,\n allow_unicode=None ,line_break=None ,\n encoding=None ,explicit_start=None ,explicit_end=None ,\n version=None ,tags=None ,sort_keys=True ):\n  Emitter.__init__(self,stream,canonical=canonical,\n  indent=indent,width=width,\n  allow_unicode=allow_unicode,line_break=line_break)\n  Serializer.__init__(self,encoding=encoding,\n  explicit_start=explicit_start,explicit_end=explicit_end,\n  version=version,tags=tags)\n  SafeRepresenter.__init__(self,default_style=default_style,\n  default_flow_style=default_flow_style,sort_keys=sort_keys)\n  Resolver.__init__(self)\n  \nclass Dumper(Emitter,Serializer,Representer,Resolver):\n\n def __init__(self,stream,\n default_style=None ,default_flow_style=False ,\n canonical=None ,indent=None ,width=None ,\n allow_unicode=None ,line_break=None ,\n encoding=None ,explicit_start=None ,explicit_end=None ,\n version=None ,tags=None ,sort_keys=True ):\n  Emitter.__init__(self,stream,canonical=canonical,\n  indent=indent,width=width,\n  allow_unicode=allow_unicode,line_break=line_break)\n  Serializer.__init__(self,encoding=encoding,\n  explicit_start=explicit_start,explicit_end=explicit_end,\n  version=version,tags=tags)\n  Representer.__init__(self,default_style=default_style,\n  default_flow_style=default_flow_style,sort_keys=sort_keys)\n  Resolver.__init__(self)\n  \n", ["yaml.emitter", "yaml.representer", "yaml.resolver", "yaml.serializer"]], "yaml.cyaml": [".py", "\n__all__=[\n'CBaseLoader','CSafeLoader','CFullLoader','CUnsafeLoader','CLoader',\n'CBaseDumper','CSafeDumper','CDumper'\n]\n\nfrom yaml._yaml import CParser,CEmitter\n\nfrom .constructor import *\n\nfrom .serializer import *\nfrom .representer import *\n\nfrom .resolver import *\n\nclass CBaseLoader(CParser,BaseConstructor,BaseResolver):\n\n def __init__(self,stream):\n  CParser.__init__(self,stream)\n  BaseConstructor.__init__(self)\n  BaseResolver.__init__(self)\n  \nclass CSafeLoader(CParser,SafeConstructor,Resolver):\n\n def __init__(self,stream):\n  CParser.__init__(self,stream)\n  SafeConstructor.__init__(self)\n  Resolver.__init__(self)\n  \nclass CFullLoader(CParser,FullConstructor,Resolver):\n\n def __init__(self,stream):\n  CParser.__init__(self,stream)\n  FullConstructor.__init__(self)\n  Resolver.__init__(self)\n  \nclass CUnsafeLoader(CParser,UnsafeConstructor,Resolver):\n\n def __init__(self,stream):\n  CParser.__init__(self,stream)\n  UnsafeConstructor.__init__(self)\n  Resolver.__init__(self)\n  \nclass CLoader(CParser,Constructor,Resolver):\n\n def __init__(self,stream):\n  CParser.__init__(self,stream)\n  Constructor.__init__(self)\n  Resolver.__init__(self)\n  \nclass CBaseDumper(CEmitter,BaseRepresenter,BaseResolver):\n\n def __init__(self,stream,\n default_style=None ,default_flow_style=False ,\n canonical=None ,indent=None ,width=None ,\n allow_unicode=None ,line_break=None ,\n encoding=None ,explicit_start=None ,explicit_end=None ,\n version=None ,tags=None ,sort_keys=True ):\n  CEmitter.__init__(self,stream,canonical=canonical,\n  indent=indent,width=width,encoding=encoding,\n  allow_unicode=allow_unicode,line_break=line_break,\n  explicit_start=explicit_start,explicit_end=explicit_end,\n  version=version,tags=tags)\n  Representer.__init__(self,default_style=default_style,\n  default_flow_style=default_flow_style,sort_keys=sort_keys)\n  Resolver.__init__(self)\n  \nclass CSafeDumper(CEmitter,SafeRepresenter,Resolver):\n\n def __init__(self,stream,\n default_style=None ,default_flow_style=False ,\n canonical=None ,indent=None ,width=None ,\n allow_unicode=None ,line_break=None ,\n encoding=None ,explicit_start=None ,explicit_end=None ,\n version=None ,tags=None ,sort_keys=True ):\n  CEmitter.__init__(self,stream,canonical=canonical,\n  indent=indent,width=width,encoding=encoding,\n  allow_unicode=allow_unicode,line_break=line_break,\n  explicit_start=explicit_start,explicit_end=explicit_end,\n  version=version,tags=tags)\n  SafeRepresenter.__init__(self,default_style=default_style,\n  default_flow_style=default_flow_style,sort_keys=sort_keys)\n  Resolver.__init__(self)\n  \nclass CDumper(CEmitter,Serializer,Representer,Resolver):\n\n def __init__(self,stream,\n default_style=None ,default_flow_style=False ,\n canonical=None ,indent=None ,width=None ,\n allow_unicode=None ,line_break=None ,\n encoding=None ,explicit_start=None ,explicit_end=None ,\n version=None ,tags=None ,sort_keys=True ):\n  CEmitter.__init__(self,stream,canonical=canonical,\n  indent=indent,width=width,encoding=encoding,\n  allow_unicode=allow_unicode,line_break=line_break,\n  explicit_start=explicit_start,explicit_end=explicit_end,\n  version=version,tags=tags)\n  Representer.__init__(self,default_style=default_style,\n  default_flow_style=default_flow_style,sort_keys=sort_keys)\n  Resolver.__init__(self)\n  \n", ["yaml._yaml", "yaml.constructor", "yaml.representer", "yaml.resolver", "yaml.serializer"]], "yaml.parser": [".py", "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n__all__=['Parser','ParserError']\n\nfrom .error import MarkedYAMLError\nfrom .tokens import *\nfrom .events import *\nfrom .scanner import *\n\nclass ParserError(MarkedYAMLError):\n pass\n \nclass Parser:\n\n\n\n DEFAULT_TAGS={\n '!':'!',\n '!!':'tag:yaml.org,2002:',\n }\n \n def __init__(self):\n  self.current_event=None\n  self.yaml_version=None\n  self.tag_handles={}\n  self.states=[]\n  self.marks=[]\n  self.state=self.parse_stream_start\n  \n def dispose(self):\n \n  self.states=[]\n  self.state=None\n  \n def check_event(self,*choices):\n \n  if self.current_event is None :\n   if self.state:\n    self.current_event=self.state()\n  if self.current_event is not None :\n   if not choices:\n    return True\n   for choice in choices:\n    if isinstance(self.current_event,choice):\n     return True\n  return False\n  \n def peek_event(self):\n \n  if self.current_event is None :\n   if self.state:\n    self.current_event=self.state()\n  return self.current_event\n  \n def get_event(self):\n \n  if self.current_event is None :\n   if self.state:\n    self.current_event=self.state()\n  value=self.current_event\n  self.current_event=None\n  return value\n  \n  \n  \n  \n  \n def parse_stream_start(self):\n \n \n  token=self.get_token()\n  event=StreamStartEvent(token.start_mark,token.end_mark,\n  encoding=token.encoding)\n  \n  \n  self.state=self.parse_implicit_document_start\n  \n  return event\n  \n def parse_implicit_document_start(self):\n \n \n  if not self.check_token(DirectiveToken,DocumentStartToken,\n  StreamEndToken):\n   self.tag_handles=self.DEFAULT_TAGS\n   token=self.peek_token()\n   start_mark=end_mark=token.start_mark\n   event=DocumentStartEvent(start_mark,end_mark,\n   explicit=False )\n   \n   \n   self.states.append(self.parse_document_end)\n   self.state=self.parse_block_node\n   \n   return event\n   \n  else :\n   return self.parse_document_start()\n   \n def parse_document_start(self):\n \n \n  while self.check_token(DocumentEndToken):\n   self.get_token()\n   \n   \n  if not self.check_token(StreamEndToken):\n   token=self.peek_token()\n   start_mark=token.start_mark\n   version,tags=self.process_directives()\n   if not self.check_token(DocumentStartToken):\n    raise ParserError(None ,None ,\n    \"expected '<document start>', but found %r\"\n    %self.peek_token().id,\n    self.peek_token().start_mark)\n   token=self.get_token()\n   end_mark=token.end_mark\n   event=DocumentStartEvent(start_mark,end_mark,\n   explicit=True ,version=version,tags=tags)\n   self.states.append(self.parse_document_end)\n   self.state=self.parse_document_content\n  else :\n  \n   token=self.get_token()\n   event=StreamEndEvent(token.start_mark,token.end_mark)\n   assert not self.states\n   assert not self.marks\n   self.state=None\n  return event\n  \n def parse_document_end(self):\n \n \n  token=self.peek_token()\n  start_mark=end_mark=token.start_mark\n  explicit=False\n  if self.check_token(DocumentEndToken):\n   token=self.get_token()\n   end_mark=token.end_mark\n   explicit=True\n  event=DocumentEndEvent(start_mark,end_mark,\n  explicit=explicit)\n  \n  \n  self.state=self.parse_document_start\n  \n  return event\n  \n def parse_document_content(self):\n  if self.check_token(DirectiveToken,\n  DocumentStartToken,DocumentEndToken,StreamEndToken):\n   event=self.process_empty_scalar(self.peek_token().start_mark)\n   self.state=self.states.pop()\n   return event\n  else :\n   return self.parse_block_node()\n   \n def process_directives(self):\n  self.yaml_version=None\n  self.tag_handles={}\n  while self.check_token(DirectiveToken):\n   token=self.get_token()\n   if token.name =='YAML':\n    if self.yaml_version is not None :\n     raise ParserError(None ,None ,\n     \"found duplicate YAML directive\",token.start_mark)\n    major,minor=token.value\n    if major !=1:\n     raise ParserError(None ,None ,\n     \"found incompatible YAML document (version 1.* is required)\",\n     token.start_mark)\n    self.yaml_version=token.value\n   elif token.name =='TAG':\n    handle,prefix=token.value\n    if handle in self.tag_handles:\n     raise ParserError(None ,None ,\n     \"duplicate tag handle %r\"%handle,\n     token.start_mark)\n    self.tag_handles[handle]=prefix\n  if self.tag_handles:\n   value=self.yaml_version,self.tag_handles.copy()\n  else :\n   value=self.yaml_version,None\n  for key in self.DEFAULT_TAGS:\n   if key not in self.tag_handles:\n    self.tag_handles[key]=self.DEFAULT_TAGS[key]\n  return value\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n def parse_block_node(self):\n  return self.parse_node(block=True )\n  \n def parse_flow_node(self):\n  return self.parse_node()\n  \n def parse_block_node_or_indentless_sequence(self):\n  return self.parse_node(block=True ,indentless_sequence=True )\n  \n def parse_node(self,block=False ,indentless_sequence=False ):\n  if self.check_token(AliasToken):\n   token=self.get_token()\n   event=AliasEvent(token.value,token.start_mark,token.end_mark)\n   self.state=self.states.pop()\n  else :\n   anchor=None\n   tag=None\n   start_mark=end_mark=tag_mark=None\n   if self.check_token(AnchorToken):\n    token=self.get_token()\n    start_mark=token.start_mark\n    end_mark=token.end_mark\n    anchor=token.value\n    if self.check_token(TagToken):\n     token=self.get_token()\n     tag_mark=token.start_mark\n     end_mark=token.end_mark\n     tag=token.value\n   elif self.check_token(TagToken):\n    token=self.get_token()\n    start_mark=tag_mark=token.start_mark\n    end_mark=token.end_mark\n    tag=token.value\n    if self.check_token(AnchorToken):\n     token=self.get_token()\n     end_mark=token.end_mark\n     anchor=token.value\n   if tag is not None :\n    handle,suffix=tag\n    if handle is not None :\n     if handle not in self.tag_handles:\n      raise ParserError(\"while parsing a node\",start_mark,\n      \"found undefined tag handle %r\"%handle,\n      tag_mark)\n     tag=self.tag_handles[handle]+suffix\n    else :\n     tag=suffix\n     \n     \n     \n     \n   if start_mark is None :\n    start_mark=end_mark=self.peek_token().start_mark\n   event=None\n   implicit=(tag is None or tag =='!')\n   if indentless_sequence and self.check_token(BlockEntryToken):\n    end_mark=self.peek_token().end_mark\n    event=SequenceStartEvent(anchor,tag,implicit,\n    start_mark,end_mark)\n    self.state=self.parse_indentless_sequence_entry\n   else :\n    if self.check_token(ScalarToken):\n     token=self.get_token()\n     end_mark=token.end_mark\n     if (token.plain and tag is None )or tag =='!':\n      implicit=(True ,False )\n     elif tag is None :\n      implicit=(False ,True )\n     else :\n      implicit=(False ,False )\n     event=ScalarEvent(anchor,tag,implicit,token.value,\n     start_mark,end_mark,style=token.style)\n     self.state=self.states.pop()\n    elif self.check_token(FlowSequenceStartToken):\n     end_mark=self.peek_token().end_mark\n     event=SequenceStartEvent(anchor,tag,implicit,\n     start_mark,end_mark,flow_style=True )\n     self.state=self.parse_flow_sequence_first_entry\n    elif self.check_token(FlowMappingStartToken):\n     end_mark=self.peek_token().end_mark\n     event=MappingStartEvent(anchor,tag,implicit,\n     start_mark,end_mark,flow_style=True )\n     self.state=self.parse_flow_mapping_first_key\n    elif block and self.check_token(BlockSequenceStartToken):\n     end_mark=self.peek_token().start_mark\n     event=SequenceStartEvent(anchor,tag,implicit,\n     start_mark,end_mark,flow_style=False )\n     self.state=self.parse_block_sequence_first_entry\n    elif block and self.check_token(BlockMappingStartToken):\n     end_mark=self.peek_token().start_mark\n     event=MappingStartEvent(anchor,tag,implicit,\n     start_mark,end_mark,flow_style=False )\n     self.state=self.parse_block_mapping_first_key\n    elif anchor is not None or tag is not None :\n    \n    \n     event=ScalarEvent(anchor,tag,(implicit,False ),'',\n     start_mark,end_mark)\n     self.state=self.states.pop()\n    else :\n     if block:\n      node='block'\n     else :\n      node='flow'\n     token=self.peek_token()\n     raise ParserError(\"while parsing a %s node\"%node,start_mark,\n     \"expected the node content, but found %r\"%token.id,\n     token.start_mark)\n  return event\n  \n  \n  \n def parse_block_sequence_first_entry(self):\n  token=self.get_token()\n  self.marks.append(token.start_mark)\n  return self.parse_block_sequence_entry()\n  \n def parse_block_sequence_entry(self):\n  if self.check_token(BlockEntryToken):\n   token=self.get_token()\n   if not self.check_token(BlockEntryToken,BlockEndToken):\n    self.states.append(self.parse_block_sequence_entry)\n    return self.parse_block_node()\n   else :\n    self.state=self.parse_block_sequence_entry\n    return self.process_empty_scalar(token.end_mark)\n  if not self.check_token(BlockEndToken):\n   token=self.peek_token()\n   raise ParserError(\"while parsing a block collection\",self.marks[-1],\n   \"expected <block end>, but found %r\"%token.id,token.start_mark)\n  token=self.get_token()\n  event=SequenceEndEvent(token.start_mark,token.end_mark)\n  self.state=self.states.pop()\n  self.marks.pop()\n  return event\n  \n  \n  \n def parse_indentless_sequence_entry(self):\n  if self.check_token(BlockEntryToken):\n   token=self.get_token()\n   if not self.check_token(BlockEntryToken,\n   KeyToken,ValueToken,BlockEndToken):\n    self.states.append(self.parse_indentless_sequence_entry)\n    return self.parse_block_node()\n   else :\n    self.state=self.parse_indentless_sequence_entry\n    return self.process_empty_scalar(token.end_mark)\n  token=self.peek_token()\n  event=SequenceEndEvent(token.start_mark,token.start_mark)\n  self.state=self.states.pop()\n  return event\n  \n  \n  \n  \n  \n  \n def parse_block_mapping_first_key(self):\n  token=self.get_token()\n  self.marks.append(token.start_mark)\n  return self.parse_block_mapping_key()\n  \n def parse_block_mapping_key(self):\n  if self.check_token(KeyToken):\n   token=self.get_token()\n   if not self.check_token(KeyToken,ValueToken,BlockEndToken):\n    self.states.append(self.parse_block_mapping_value)\n    return self.parse_block_node_or_indentless_sequence()\n   else :\n    self.state=self.parse_block_mapping_value\n    return self.process_empty_scalar(token.end_mark)\n  if not self.check_token(BlockEndToken):\n   token=self.peek_token()\n   raise ParserError(\"while parsing a block mapping\",self.marks[-1],\n   \"expected <block end>, but found %r\"%token.id,token.start_mark)\n  token=self.get_token()\n  event=MappingEndEvent(token.start_mark,token.end_mark)\n  self.state=self.states.pop()\n  self.marks.pop()\n  return event\n  \n def parse_block_mapping_value(self):\n  if self.check_token(ValueToken):\n   token=self.get_token()\n   if not self.check_token(KeyToken,ValueToken,BlockEndToken):\n    self.states.append(self.parse_block_mapping_key)\n    return self.parse_block_node_or_indentless_sequence()\n   else :\n    self.state=self.parse_block_mapping_key\n    return self.process_empty_scalar(token.end_mark)\n  else :\n   self.state=self.parse_block_mapping_key\n   token=self.peek_token()\n   return self.process_empty_scalar(token.start_mark)\n   \n   \n   \n   \n   \n   \n   \n   \n   \n   \n   \n   \n def parse_flow_sequence_first_entry(self):\n  token=self.get_token()\n  self.marks.append(token.start_mark)\n  return self.parse_flow_sequence_entry(first=True )\n  \n def parse_flow_sequence_entry(self,first=False ):\n  if not self.check_token(FlowSequenceEndToken):\n   if not first:\n    if self.check_token(FlowEntryToken):\n     self.get_token()\n    else :\n     token=self.peek_token()\n     raise ParserError(\"while parsing a flow sequence\",self.marks[-1],\n     \"expected ',' or ']', but got %r\"%token.id,token.start_mark)\n     \n   if self.check_token(KeyToken):\n    token=self.peek_token()\n    event=MappingStartEvent(None ,None ,True ,\n    token.start_mark,token.end_mark,\n    flow_style=True )\n    self.state=self.parse_flow_sequence_entry_mapping_key\n    return event\n   elif not self.check_token(FlowSequenceEndToken):\n    self.states.append(self.parse_flow_sequence_entry)\n    return self.parse_flow_node()\n  token=self.get_token()\n  event=SequenceEndEvent(token.start_mark,token.end_mark)\n  self.state=self.states.pop()\n  self.marks.pop()\n  return event\n  \n def parse_flow_sequence_entry_mapping_key(self):\n  token=self.get_token()\n  if not self.check_token(ValueToken,\n  FlowEntryToken,FlowSequenceEndToken):\n   self.states.append(self.parse_flow_sequence_entry_mapping_value)\n   return self.parse_flow_node()\n  else :\n   self.state=self.parse_flow_sequence_entry_mapping_value\n   return self.process_empty_scalar(token.end_mark)\n   \n def parse_flow_sequence_entry_mapping_value(self):\n  if self.check_token(ValueToken):\n   token=self.get_token()\n   if not self.check_token(FlowEntryToken,FlowSequenceEndToken):\n    self.states.append(self.parse_flow_sequence_entry_mapping_end)\n    return self.parse_flow_node()\n   else :\n    self.state=self.parse_flow_sequence_entry_mapping_end\n    return self.process_empty_scalar(token.end_mark)\n  else :\n   self.state=self.parse_flow_sequence_entry_mapping_end\n   token=self.peek_token()\n   return self.process_empty_scalar(token.start_mark)\n   \n def parse_flow_sequence_entry_mapping_end(self):\n  self.state=self.parse_flow_sequence_entry\n  token=self.peek_token()\n  return MappingEndEvent(token.start_mark,token.start_mark)\n  \n  \n  \n  \n  \n  \n  \n def parse_flow_mapping_first_key(self):\n  token=self.get_token()\n  self.marks.append(token.start_mark)\n  return self.parse_flow_mapping_key(first=True )\n  \n def parse_flow_mapping_key(self,first=False ):\n  if not self.check_token(FlowMappingEndToken):\n   if not first:\n    if self.check_token(FlowEntryToken):\n     self.get_token()\n    else :\n     token=self.peek_token()\n     raise ParserError(\"while parsing a flow mapping\",self.marks[-1],\n     \"expected ',' or '}', but got %r\"%token.id,token.start_mark)\n   if self.check_token(KeyToken):\n    token=self.get_token()\n    if not self.check_token(ValueToken,\n    FlowEntryToken,FlowMappingEndToken):\n     self.states.append(self.parse_flow_mapping_value)\n     return self.parse_flow_node()\n    else :\n     self.state=self.parse_flow_mapping_value\n     return self.process_empty_scalar(token.end_mark)\n   elif not self.check_token(FlowMappingEndToken):\n    self.states.append(self.parse_flow_mapping_empty_value)\n    return self.parse_flow_node()\n  token=self.get_token()\n  event=MappingEndEvent(token.start_mark,token.end_mark)\n  self.state=self.states.pop()\n  self.marks.pop()\n  return event\n  \n def parse_flow_mapping_value(self):\n  if self.check_token(ValueToken):\n   token=self.get_token()\n   if not self.check_token(FlowEntryToken,FlowMappingEndToken):\n    self.states.append(self.parse_flow_mapping_key)\n    return self.parse_flow_node()\n   else :\n    self.state=self.parse_flow_mapping_key\n    return self.process_empty_scalar(token.end_mark)\n  else :\n   self.state=self.parse_flow_mapping_key\n   token=self.peek_token()\n   return self.process_empty_scalar(token.start_mark)\n   \n def parse_flow_mapping_empty_value(self):\n  self.state=self.parse_flow_mapping_key\n  return self.process_empty_scalar(self.peek_token().start_mark)\n  \n def process_empty_scalar(self,mark):\n  return ScalarEvent(None ,None ,(True ,False ),'',mark,mark)\n  \n", ["yaml.error", "yaml.events", "yaml.scanner", "yaml.tokens"]], "yaml.reader": [".py", "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n__all__=['Reader','ReaderError']\n\nfrom .error import YAMLError,Mark\n\nimport codecs,re\n\nclass ReaderError(YAMLError):\n\n def __init__(self,name,position,character,encoding,reason):\n  self.name=name\n  self.character=character\n  self.position=position\n  self.encoding=encoding\n  self.reason=reason\n  \n def __str__(self):\n  if isinstance(self.character,bytes):\n   return \"'%s' codec can't decode byte #x%02x: %s\\n\"\\\n   \"  in \\\"%s\\\", position %d\"\\\n   %(self.encoding,ord(self.character),self.reason,\n   self.name,self.position)\n  else :\n   return \"unacceptable character #x%04x: %s\\n\"\\\n   \"  in \\\"%s\\\", position %d\"\\\n   %(self.character,self.reason,\n   self.name,self.position)\n   \nclass Reader(object):\n\n\n\n\n\n\n\n\n\n\n\n\n\n def __init__(self,stream):\n  self.name=None\n  self.stream=None\n  self.stream_pointer=0\n  self.eof=True\n  self.buffer=''\n  self.pointer=0\n  self.raw_buffer=None\n  self.raw_decode=None\n  self.encoding=None\n  self.index=0\n  self.line=0\n  self.column=0\n  if isinstance(stream,str):\n   self.name=\"<unicode string>\"\n   self.check_printable(stream)\n   self.buffer=stream+'\\0'\n  elif isinstance(stream,bytes):\n   self.name=\"<byte string>\"\n   self.raw_buffer=stream\n   self.determine_encoding()\n  else :\n   self.stream=stream\n   self.name=getattr(stream,'name',\"<file>\")\n   self.eof=False\n   self.raw_buffer=None\n   self.determine_encoding()\n   \n def peek(self,index=0):\n  try :\n   return self.buffer[self.pointer+index]\n  except IndexError:\n   self.update(index+1)\n   return self.buffer[self.pointer+index]\n   \n def prefix(self,length=1):\n  if self.pointer+length >=len(self.buffer):\n   self.update(length)\n  return self.buffer[self.pointer:self.pointer+length]\n  \n def forward(self,length=1):\n  if self.pointer+length+1 >=len(self.buffer):\n   self.update(length+1)\n  while length:\n   ch=self.buffer[self.pointer]\n   self.pointer +=1\n   self.index +=1\n   if ch in '\\n\\x85\\u2028\\u2029'\\\n   or (ch =='\\r'and self.buffer[self.pointer]!='\\n'):\n    self.line +=1\n    self.column=0\n   elif ch !='\\uFEFF':\n    self.column +=1\n   length -=1\n   \n def get_mark(self):\n  if self.stream is None :\n   return Mark(self.name,self.index,self.line,self.column,\n   self.buffer,self.pointer)\n  else :\n   return Mark(self.name,self.index,self.line,self.column,\n   None ,None )\n   \n def determine_encoding(self):\n  while not self.eof and (self.raw_buffer is None or len(self.raw_buffer)<2):\n   self.update_raw()\n  if isinstance(self.raw_buffer,bytes):\n   if self.raw_buffer.startswith(codecs.BOM_UTF16_LE):\n    self.raw_decode=codecs.utf_16_le_decode\n    self.encoding='utf-16-le'\n   elif self.raw_buffer.startswith(codecs.BOM_UTF16_BE):\n    self.raw_decode=codecs.utf_16_be_decode\n    self.encoding='utf-16-be'\n   else :\n    self.raw_decode=codecs.utf_8_decode\n    self.encoding='utf-8'\n  self.update(1)\n  \n NON_PRINTABLE=re.compile('[^\\x09\\x0A\\x0D\\x20-\\x7E\\x85\\xA0-\\uD7FF\\uE000-\\uFFFD\\U00010000-\\U0010ffff]')\n def check_printable(self,data):\n  match=self.NON_PRINTABLE.search(data)\n  if match:\n   character=match.group()\n   position=self.index+(len(self.buffer)-self.pointer)+match.start()\n   raise ReaderError(self.name,position,ord(character),\n   'unicode',\"special characters are not allowed\")\n   \n def update(self,length):\n  if self.raw_buffer is None :\n   return\n  self.buffer=self.buffer[self.pointer:]\n  self.pointer=0\n  while len(self.buffer)<length:\n   if not self.eof:\n    self.update_raw()\n   if self.raw_decode is not None :\n    try :\n     data,converted=self.raw_decode(self.raw_buffer,\n     'strict',self.eof)\n    except UnicodeDecodeError as exc:\n     character=self.raw_buffer[exc.start]\n     if self.stream is not None :\n      position=self.stream_pointer -len(self.raw_buffer)+exc.start\n     else :\n      position=exc.start\n     raise ReaderError(self.name,position,character,\n     exc.encoding,exc.reason)\n   else :\n    data=self.raw_buffer\n    converted=len(data)\n   self.check_printable(data)\n   self.buffer +=data\n   self.raw_buffer=self.raw_buffer[converted:]\n   if self.eof:\n    self.buffer +='\\0'\n    self.raw_buffer=None\n    break\n    \n def update_raw(self,size=4096):\n  data=self.stream.read(size)\n  if self.raw_buffer is None :\n   self.raw_buffer=data\n  else :\n   self.raw_buffer +=data\n  self.stream_pointer +=len(data)\n  if not data:\n   self.eof=True\n", ["codecs", "re", "yaml.error"]], "yaml.loader": [".py", "\n__all__=['BaseLoader','FullLoader','SafeLoader','Loader','UnsafeLoader']\n\nfrom .reader import *\nfrom .scanner import *\nfrom .parser import *\nfrom .composer import *\nfrom .constructor import *\nfrom .resolver import *\n\nclass BaseLoader(Reader,Scanner,Parser,Composer,BaseConstructor,BaseResolver):\n\n def __init__(self,stream):\n  Reader.__init__(self,stream)\n  Scanner.__init__(self)\n  Parser.__init__(self)\n  Composer.__init__(self)\n  BaseConstructor.__init__(self)\n  BaseResolver.__init__(self)\n  \nclass FullLoader(Reader,Scanner,Parser,Composer,FullConstructor,Resolver):\n\n def __init__(self,stream):\n  Reader.__init__(self,stream)\n  Scanner.__init__(self)\n  Parser.__init__(self)\n  Composer.__init__(self)\n  FullConstructor.__init__(self)\n  Resolver.__init__(self)\n  \nclass SafeLoader(Reader,Scanner,Parser,Composer,SafeConstructor,Resolver):\n\n def __init__(self,stream):\n  Reader.__init__(self,stream)\n  Scanner.__init__(self)\n  Parser.__init__(self)\n  Composer.__init__(self)\n  SafeConstructor.__init__(self)\n  Resolver.__init__(self)\n  \nclass Loader(Reader,Scanner,Parser,Composer,Constructor,Resolver):\n\n def __init__(self,stream):\n  Reader.__init__(self,stream)\n  Scanner.__init__(self)\n  Parser.__init__(self)\n  Composer.__init__(self)\n  Constructor.__init__(self)\n  Resolver.__init__(self)\n  \n  \n  \n  \n  \nclass UnsafeLoader(Reader,Scanner,Parser,Composer,Constructor,Resolver):\n\n def __init__(self,stream):\n  Reader.__init__(self,stream)\n  Scanner.__init__(self)\n  Parser.__init__(self)\n  Composer.__init__(self)\n  Constructor.__init__(self)\n  Resolver.__init__(self)\n", ["yaml.composer", "yaml.constructor", "yaml.parser", "yaml.reader", "yaml.resolver", "yaml.scanner"]], "yaml.resolver": [".py", "\n__all__=['BaseResolver','Resolver']\n\nfrom .error import *\nfrom .nodes import *\n\nimport re\n\nclass ResolverError(YAMLError):\n pass\n \nclass BaseResolver:\n\n DEFAULT_SCALAR_TAG='tag:yaml.org,2002:str'\n DEFAULT_SEQUENCE_TAG='tag:yaml.org,2002:seq'\n DEFAULT_MAPPING_TAG='tag:yaml.org,2002:map'\n \n yaml_implicit_resolvers={}\n yaml_path_resolvers={}\n \n def __init__(self):\n  self.resolver_exact_paths=[]\n  self.resolver_prefix_paths=[]\n  \n @classmethod\n def add_implicit_resolver(cls,tag,regexp,first):\n  if not 'yaml_implicit_resolvers'in cls.__dict__:\n   implicit_resolvers={}\n   for key in cls.yaml_implicit_resolvers:\n    implicit_resolvers[key]=cls.yaml_implicit_resolvers[key][:]\n   cls.yaml_implicit_resolvers=implicit_resolvers\n  if first is None :\n   first=[None ]\n  for ch in first:\n   cls.yaml_implicit_resolvers.setdefault(ch,[]).append((tag,regexp))\n   \n @classmethod\n def add_path_resolver(cls,tag,path,kind=None ):\n \n \n \n \n \n \n \n \n \n \n \n \n  if not 'yaml_path_resolvers'in cls.__dict__:\n   cls.yaml_path_resolvers=cls.yaml_path_resolvers.copy()\n  new_path=[]\n  for element in path:\n   if isinstance(element,(list,tuple)):\n    if len(element)==2:\n     node_check,index_check=element\n    elif len(element)==1:\n     node_check=element[0]\n     index_check=True\n    else :\n     raise ResolverError(\"Invalid path element: %s\"%element)\n   else :\n    node_check=None\n    index_check=element\n   if node_check is str:\n    node_check=ScalarNode\n   elif node_check is list:\n    node_check=SequenceNode\n   elif node_check is dict:\n    node_check=MappingNode\n   elif node_check not in [ScalarNode,SequenceNode,MappingNode]\\\n   and not isinstance(node_check,str)\\\n   and node_check is not None :\n    raise ResolverError(\"Invalid node checker: %s\"%node_check)\n   if not isinstance(index_check,(str,int))\\\n   and index_check is not None :\n    raise ResolverError(\"Invalid index checker: %s\"%index_check)\n   new_path.append((node_check,index_check))\n  if kind is str:\n   kind=ScalarNode\n  elif kind is list:\n   kind=SequenceNode\n  elif kind is dict:\n   kind=MappingNode\n  elif kind not in [ScalarNode,SequenceNode,MappingNode]\\\n  and kind is not None :\n   raise ResolverError(\"Invalid node kind: %s\"%kind)\n  cls.yaml_path_resolvers[tuple(new_path),kind]=tag\n  \n def descend_resolver(self,current_node,current_index):\n  if not self.yaml_path_resolvers:\n   return\n  exact_paths={}\n  prefix_paths=[]\n  if current_node:\n   depth=len(self.resolver_prefix_paths)\n   for path,kind in self.resolver_prefix_paths[-1]:\n    if self.check_resolver_prefix(depth,path,kind,\n    current_node,current_index):\n     if len(path)>depth:\n      prefix_paths.append((path,kind))\n     else :\n      exact_paths[kind]=self.yaml_path_resolvers[path,kind]\n  else :\n   for path,kind in self.yaml_path_resolvers:\n    if not path:\n     exact_paths[kind]=self.yaml_path_resolvers[path,kind]\n    else :\n     prefix_paths.append((path,kind))\n  self.resolver_exact_paths.append(exact_paths)\n  self.resolver_prefix_paths.append(prefix_paths)\n  \n def ascend_resolver(self):\n  if not self.yaml_path_resolvers:\n   return\n  self.resolver_exact_paths.pop()\n  self.resolver_prefix_paths.pop()\n  \n def check_resolver_prefix(self,depth,path,kind,\n current_node,current_index):\n  node_check,index_check=path[depth -1]\n  if isinstance(node_check,str):\n   if current_node.tag !=node_check:\n    return\n  elif node_check is not None :\n   if not isinstance(current_node,node_check):\n    return\n  if index_check is True and current_index is not None :\n   return\n  if (index_check is False or index_check is None )\\\n  and current_index is None :\n   return\n  if isinstance(index_check,str):\n   if not (isinstance(current_index,ScalarNode)\n   and index_check ==current_index.value):\n    return\n  elif isinstance(index_check,int)and not isinstance(index_check,bool):\n   if index_check !=current_index:\n    return\n  return True\n  \n def resolve(self,kind,value,implicit):\n  if kind is ScalarNode and implicit[0]:\n   if value =='':\n    resolvers=self.yaml_implicit_resolvers.get('',[])\n   else :\n    resolvers=self.yaml_implicit_resolvers.get(value[0],[])\n   wildcard_resolvers=self.yaml_implicit_resolvers.get(None ,[])\n   for tag,regexp in resolvers+wildcard_resolvers:\n    if regexp.match(value):\n     return tag\n   implicit=implicit[1]\n  if self.yaml_path_resolvers:\n   exact_paths=self.resolver_exact_paths[-1]\n   if kind in exact_paths:\n    return exact_paths[kind]\n   if None in exact_paths:\n    return exact_paths[None ]\n  if kind is ScalarNode:\n   return self.DEFAULT_SCALAR_TAG\n  elif kind is SequenceNode:\n   return self.DEFAULT_SEQUENCE_TAG\n  elif kind is MappingNode:\n   return self.DEFAULT_MAPPING_TAG\n   \nclass Resolver(BaseResolver):\n pass\n \nResolver.add_implicit_resolver(\n'tag:yaml.org,2002:bool',\nre.compile(r'''^(?:yes|Yes|YES|no|No|NO\n                    |true|True|TRUE|false|False|FALSE\n                    |on|On|ON|off|Off|OFF)$''',re.X),\nlist('yYnNtTfFoO'))\n\nResolver.add_implicit_resolver(\n'tag:yaml.org,2002:float',\nre.compile(r'''^(?:[-+]?(?:[0-9][0-9_]*)\\.[0-9_]*(?:[eE][-+][0-9]+)?\n                    |\\.[0-9][0-9_]*(?:[eE][-+][0-9]+)?\n                    |[-+]?[0-9][0-9_]*(?::[0-5]?[0-9])+\\.[0-9_]*\n                    |[-+]?\\.(?:inf|Inf|INF)\n                    |\\.(?:nan|NaN|NAN))$''',re.X),\nlist('-+0123456789.'))\n\nResolver.add_implicit_resolver(\n'tag:yaml.org,2002:int',\nre.compile(r'''^(?:[-+]?0b[0-1_]+\n                    |[-+]?0[0-7_]+\n                    |[-+]?(?:0|[1-9][0-9_]*)\n                    |[-+]?0x[0-9a-fA-F_]+\n                    |[-+]?[1-9][0-9_]*(?::[0-5]?[0-9])+)$''',re.X),\nlist('-+0123456789'))\n\nResolver.add_implicit_resolver(\n'tag:yaml.org,2002:merge',\nre.compile(r'^(?:<<)$'),\n['<'])\n\nResolver.add_implicit_resolver(\n'tag:yaml.org,2002:null',\nre.compile(r'''^(?: ~\n                    |null|Null|NULL\n                    | )$''',re.X),\n['~','n','N',''])\n\nResolver.add_implicit_resolver(\n'tag:yaml.org,2002:timestamp',\nre.compile(r'''^(?:[0-9][0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9]\n                    |[0-9][0-9][0-9][0-9] -[0-9][0-9]? -[0-9][0-9]?\n                     (?:[Tt]|[ \\t]+)[0-9][0-9]?\n                     :[0-9][0-9] :[0-9][0-9] (?:\\.[0-9]*)?\n                     (?:[ \\t]*(?:Z|[-+][0-9][0-9]?(?::[0-9][0-9])?))?)$''',re.X),\nlist('0123456789'))\n\nResolver.add_implicit_resolver(\n'tag:yaml.org,2002:value',\nre.compile(r'^(?:=)$'),\n['='])\n\n\n\nResolver.add_implicit_resolver(\n'tag:yaml.org,2002:yaml',\nre.compile(r'^(?:!|&|\\*)$'),\nlist('!&*'))\n\n", ["re", "yaml.error", "yaml.nodes"]], "yaml.serializer": [".py", "\n__all__=['Serializer','SerializerError']\n\nfrom .error import YAMLError\nfrom .events import *\nfrom .nodes import *\n\nclass SerializerError(YAMLError):\n pass\n \nclass Serializer:\n\n ANCHOR_TEMPLATE='id%03d'\n \n def __init__(self,encoding=None ,\n explicit_start=None ,explicit_end=None ,version=None ,tags=None ):\n  self.use_encoding=encoding\n  self.use_explicit_start=explicit_start\n  self.use_explicit_end=explicit_end\n  self.use_version=version\n  self.use_tags=tags\n  self.serialized_nodes={}\n  self.anchors={}\n  self.last_anchor_id=0\n  self.closed=None\n  \n def open(self):\n  if self.closed is None :\n   self.emit(StreamStartEvent(encoding=self.use_encoding))\n   self.closed=False\n  elif self.closed:\n   raise SerializerError(\"serializer is closed\")\n  else :\n   raise SerializerError(\"serializer is already opened\")\n   \n def close(self):\n  if self.closed is None :\n   raise SerializerError(\"serializer is not opened\")\n  elif not self.closed:\n   self.emit(StreamEndEvent())\n   self.closed=True\n   \n   \n   \n   \n def serialize(self,node):\n  if self.closed is None :\n   raise SerializerError(\"serializer is not opened\")\n  elif self.closed:\n   raise SerializerError(\"serializer is closed\")\n  self.emit(DocumentStartEvent(explicit=self.use_explicit_start,\n  version=self.use_version,tags=self.use_tags))\n  self.anchor_node(node)\n  self.serialize_node(node,None ,None )\n  self.emit(DocumentEndEvent(explicit=self.use_explicit_end))\n  self.serialized_nodes={}\n  self.anchors={}\n  self.last_anchor_id=0\n  \n def anchor_node(self,node):\n  if node in self.anchors:\n   if self.anchors[node]is None :\n    self.anchors[node]=self.generate_anchor(node)\n  else :\n   self.anchors[node]=None\n   if isinstance(node,SequenceNode):\n    for item in node.value:\n     self.anchor_node(item)\n   elif isinstance(node,MappingNode):\n    for key,value in node.value:\n     self.anchor_node(key)\n     self.anchor_node(value)\n     \n def generate_anchor(self,node):\n  self.last_anchor_id +=1\n  return self.ANCHOR_TEMPLATE %self.last_anchor_id\n  \n def serialize_node(self,node,parent,index):\n  alias=self.anchors[node]\n  if node in self.serialized_nodes:\n   self.emit(AliasEvent(alias))\n  else :\n   self.serialized_nodes[node]=True\n   self.descend_resolver(parent,index)\n   if isinstance(node,ScalarNode):\n    detected_tag=self.resolve(ScalarNode,node.value,(True ,False ))\n    default_tag=self.resolve(ScalarNode,node.value,(False ,True ))\n    implicit=(node.tag ==detected_tag),(node.tag ==default_tag)\n    self.emit(ScalarEvent(alias,node.tag,implicit,node.value,\n    style=node.style))\n   elif isinstance(node,SequenceNode):\n    implicit=(node.tag\n    ==self.resolve(SequenceNode,node.value,True ))\n    self.emit(SequenceStartEvent(alias,node.tag,implicit,\n    flow_style=node.flow_style))\n    index=0\n    for item in node.value:\n     self.serialize_node(item,node,index)\n     index +=1\n    self.emit(SequenceEndEvent())\n   elif isinstance(node,MappingNode):\n    implicit=(node.tag\n    ==self.resolve(MappingNode,node.value,True ))\n    self.emit(MappingStartEvent(alias,node.tag,implicit,\n    flow_style=node.flow_style))\n    for key,value in node.value:\n     self.serialize_node(key,node,None )\n     self.serialize_node(value,node,key)\n    self.emit(MappingEndEvent())\n   self.ascend_resolver()\n   \n", ["yaml.error", "yaml.events", "yaml.nodes"]], "yaml.nodes": [".py", "\nclass Node(object):\n def __init__(self,tag,value,start_mark,end_mark):\n  self.tag=tag\n  self.value=value\n  self.start_mark=start_mark\n  self.end_mark=end_mark\n def __repr__(self):\n  value=self.value\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  value=repr(value)\n  return '%s(tag=%r, value=%s)'%(self.__class__.__name__,self.tag,value)\n  \nclass ScalarNode(Node):\n id='scalar'\n def __init__(self,tag,value,\n start_mark=None ,end_mark=None ,style=None ):\n  self.tag=tag\n  self.value=value\n  self.start_mark=start_mark\n  self.end_mark=end_mark\n  self.style=style\n  \nclass CollectionNode(Node):\n def __init__(self,tag,value,\n start_mark=None ,end_mark=None ,flow_style=None ):\n  self.tag=tag\n  self.value=value\n  self.start_mark=start_mark\n  self.end_mark=end_mark\n  self.flow_style=flow_style\n  \nclass SequenceNode(CollectionNode):\n id='sequence'\n \nclass MappingNode(CollectionNode):\n id='mapping'\n \n", []], "yaml.emitter": [".py", "\n\n\n\n\n\n\n\n__all__=['Emitter','EmitterError']\n\nfrom .error import YAMLError\nfrom .events import *\n\nclass EmitterError(YAMLError):\n pass\n \nclass ScalarAnalysis:\n def __init__(self,scalar,empty,multiline,\n allow_flow_plain,allow_block_plain,\n allow_single_quoted,allow_double_quoted,\n allow_block):\n  self.scalar=scalar\n  self.empty=empty\n  self.multiline=multiline\n  self.allow_flow_plain=allow_flow_plain\n  self.allow_block_plain=allow_block_plain\n  self.allow_single_quoted=allow_single_quoted\n  self.allow_double_quoted=allow_double_quoted\n  self.allow_block=allow_block\n  \nclass Emitter:\n\n DEFAULT_TAG_PREFIXES={\n '!':'!',\n 'tag:yaml.org,2002:':'!!',\n }\n \n def __init__(self,stream,canonical=None ,indent=None ,width=None ,\n allow_unicode=None ,line_break=None ):\n \n \n  self.stream=stream\n  \n  \n  self.encoding=None\n  \n  \n  \n  self.states=[]\n  self.state=self.expect_stream_start\n  \n  \n  self.events=[]\n  self.event=None\n  \n  \n  self.indents=[]\n  self.indent=None\n  \n  \n  self.flow_level=0\n  \n  \n  self.root_context=False\n  self.sequence_context=False\n  self.mapping_context=False\n  self.simple_key_context=False\n  \n  \n  \n  \n  \n  \n  self.line=0\n  self.column=0\n  self.whitespace=True\n  self.indention=True\n  \n  \n  self.open_ended=False\n  \n  \n  self.canonical=canonical\n  self.allow_unicode=allow_unicode\n  self.best_indent=2\n  if indent and 1 <indent <10:\n   self.best_indent=indent\n  self.best_width=80\n  if width and width >self.best_indent *2:\n   self.best_width=width\n  self.best_line_break='\\n'\n  if line_break in ['\\r','\\n','\\r\\n']:\n   self.best_line_break=line_break\n   \n   \n  self.tag_prefixes=None\n  \n  \n  self.prepared_anchor=None\n  self.prepared_tag=None\n  \n  \n  self.analysis=None\n  self.style=None\n  \n def dispose(self):\n \n  self.states=[]\n  self.state=None\n  \n def emit(self,event):\n  self.events.append(event)\n  while not self.need_more_events():\n   self.event=self.events.pop(0)\n   self.state()\n   self.event=None\n   \n   \n   \n def need_more_events(self):\n  if not self.events:\n   return True\n  event=self.events[0]\n  if isinstance(event,DocumentStartEvent):\n   return self.need_events(1)\n  elif isinstance(event,SequenceStartEvent):\n   return self.need_events(2)\n  elif isinstance(event,MappingStartEvent):\n   return self.need_events(3)\n  else :\n   return False\n   \n def need_events(self,count):\n  level=0\n  for event in self.events[1:]:\n   if isinstance(event,(DocumentStartEvent,CollectionStartEvent)):\n    level +=1\n   elif isinstance(event,(DocumentEndEvent,CollectionEndEvent)):\n    level -=1\n   elif isinstance(event,StreamEndEvent):\n    level=-1\n   if level <0:\n    return False\n  return (len(self.events)<count+1)\n  \n def increase_indent(self,flow=False ,indentless=False ):\n  self.indents.append(self.indent)\n  if self.indent is None :\n   if flow:\n    self.indent=self.best_indent\n   else :\n    self.indent=0\n  elif not indentless:\n   self.indent +=self.best_indent\n   \n   \n   \n   \n   \n def expect_stream_start(self):\n  if isinstance(self.event,StreamStartEvent):\n   if self.event.encoding and not hasattr(self.stream,'encoding'):\n    self.encoding=self.event.encoding\n   self.write_stream_start()\n   self.state=self.expect_first_document_start\n  else :\n   raise EmitterError(\"expected StreamStartEvent, but got %s\"\n   %self.event)\n   \n def expect_nothing(self):\n  raise EmitterError(\"expected nothing, but got %s\"%self.event)\n  \n  \n  \n def expect_first_document_start(self):\n  return self.expect_document_start(first=True )\n  \n def expect_document_start(self,first=False ):\n  if isinstance(self.event,DocumentStartEvent):\n   if (self.event.version or self.event.tags)and self.open_ended:\n    self.write_indicator('...',True )\n    self.write_indent()\n   if self.event.version:\n    version_text=self.prepare_version(self.event.version)\n    self.write_version_directive(version_text)\n   self.tag_prefixes=self.DEFAULT_TAG_PREFIXES.copy()\n   if self.event.tags:\n    handles=sorted(self.event.tags.keys())\n    for handle in handles:\n     prefix=self.event.tags[handle]\n     self.tag_prefixes[prefix]=handle\n     handle_text=self.prepare_tag_handle(handle)\n     prefix_text=self.prepare_tag_prefix(prefix)\n     self.write_tag_directive(handle_text,prefix_text)\n   implicit=(first and not self.event.explicit and not self.canonical\n   and not self.event.version and not self.event.tags\n   and not self.check_empty_document())\n   if not implicit:\n    self.write_indent()\n    self.write_indicator('---',True )\n    if self.canonical:\n     self.write_indent()\n   self.state=self.expect_document_root\n  elif isinstance(self.event,StreamEndEvent):\n   if self.open_ended:\n    self.write_indicator('...',True )\n    self.write_indent()\n   self.write_stream_end()\n   self.state=self.expect_nothing\n  else :\n   raise EmitterError(\"expected DocumentStartEvent, but got %s\"\n   %self.event)\n   \n def expect_document_end(self):\n  if isinstance(self.event,DocumentEndEvent):\n   self.write_indent()\n   if self.event.explicit:\n    self.write_indicator('...',True )\n    self.write_indent()\n   self.flush_stream()\n   self.state=self.expect_document_start\n  else :\n   raise EmitterError(\"expected DocumentEndEvent, but got %s\"\n   %self.event)\n   \n def expect_document_root(self):\n  self.states.append(self.expect_document_end)\n  self.expect_node(root=True )\n  \n  \n  \n def expect_node(self,root=False ,sequence=False ,mapping=False ,\n simple_key=False ):\n  self.root_context=root\n  self.sequence_context=sequence\n  self.mapping_context=mapping\n  self.simple_key_context=simple_key\n  if isinstance(self.event,AliasEvent):\n   self.expect_alias()\n  elif isinstance(self.event,(ScalarEvent,CollectionStartEvent)):\n   self.process_anchor('&')\n   self.process_tag()\n   if isinstance(self.event,ScalarEvent):\n    self.expect_scalar()\n   elif isinstance(self.event,SequenceStartEvent):\n    if self.flow_level or self.canonical or self.event.flow_style\\\n    or self.check_empty_sequence():\n     self.expect_flow_sequence()\n    else :\n     self.expect_block_sequence()\n   elif isinstance(self.event,MappingStartEvent):\n    if self.flow_level or self.canonical or self.event.flow_style\\\n    or self.check_empty_mapping():\n     self.expect_flow_mapping()\n    else :\n     self.expect_block_mapping()\n  else :\n   raise EmitterError(\"expected NodeEvent, but got %s\"%self.event)\n   \n def expect_alias(self):\n  if self.event.anchor is None :\n   raise EmitterError(\"anchor is not specified for alias\")\n  self.process_anchor('*')\n  self.state=self.states.pop()\n  \n def expect_scalar(self):\n  self.increase_indent(flow=True )\n  self.process_scalar()\n  self.indent=self.indents.pop()\n  self.state=self.states.pop()\n  \n  \n  \n def expect_flow_sequence(self):\n  self.write_indicator('[',True ,whitespace=True )\n  self.flow_level +=1\n  self.increase_indent(flow=True )\n  self.state=self.expect_first_flow_sequence_item\n  \n def expect_first_flow_sequence_item(self):\n  if isinstance(self.event,SequenceEndEvent):\n   self.indent=self.indents.pop()\n   self.flow_level -=1\n   self.write_indicator(']',False )\n   self.state=self.states.pop()\n  else :\n   if self.canonical or self.column >self.best_width:\n    self.write_indent()\n   self.states.append(self.expect_flow_sequence_item)\n   self.expect_node(sequence=True )\n   \n def expect_flow_sequence_item(self):\n  if isinstance(self.event,SequenceEndEvent):\n   self.indent=self.indents.pop()\n   self.flow_level -=1\n   if self.canonical:\n    self.write_indicator(',',False )\n    self.write_indent()\n   self.write_indicator(']',False )\n   self.state=self.states.pop()\n  else :\n   self.write_indicator(',',False )\n   if self.canonical or self.column >self.best_width:\n    self.write_indent()\n   self.states.append(self.expect_flow_sequence_item)\n   self.expect_node(sequence=True )\n   \n   \n   \n def expect_flow_mapping(self):\n  self.write_indicator('{',True ,whitespace=True )\n  self.flow_level +=1\n  self.increase_indent(flow=True )\n  self.state=self.expect_first_flow_mapping_key\n  \n def expect_first_flow_mapping_key(self):\n  if isinstance(self.event,MappingEndEvent):\n   self.indent=self.indents.pop()\n   self.flow_level -=1\n   self.write_indicator('}',False )\n   self.state=self.states.pop()\n  else :\n   if self.canonical or self.column >self.best_width:\n    self.write_indent()\n   if not self.canonical and self.check_simple_key():\n    self.states.append(self.expect_flow_mapping_simple_value)\n    self.expect_node(mapping=True ,simple_key=True )\n   else :\n    self.write_indicator('?',True )\n    self.states.append(self.expect_flow_mapping_value)\n    self.expect_node(mapping=True )\n    \n def expect_flow_mapping_key(self):\n  if isinstance(self.event,MappingEndEvent):\n   self.indent=self.indents.pop()\n   self.flow_level -=1\n   if self.canonical:\n    self.write_indicator(',',False )\n    self.write_indent()\n   self.write_indicator('}',False )\n   self.state=self.states.pop()\n  else :\n   self.write_indicator(',',False )\n   if self.canonical or self.column >self.best_width:\n    self.write_indent()\n   if not self.canonical and self.check_simple_key():\n    self.states.append(self.expect_flow_mapping_simple_value)\n    self.expect_node(mapping=True ,simple_key=True )\n   else :\n    self.write_indicator('?',True )\n    self.states.append(self.expect_flow_mapping_value)\n    self.expect_node(mapping=True )\n    \n def expect_flow_mapping_simple_value(self):\n  self.write_indicator(':',False )\n  self.states.append(self.expect_flow_mapping_key)\n  self.expect_node(mapping=True )\n  \n def expect_flow_mapping_value(self):\n  if self.canonical or self.column >self.best_width:\n   self.write_indent()\n  self.write_indicator(':',True )\n  self.states.append(self.expect_flow_mapping_key)\n  self.expect_node(mapping=True )\n  \n  \n  \n def expect_block_sequence(self):\n  indentless=(self.mapping_context and not self.indention)\n  self.increase_indent(flow=False ,indentless=indentless)\n  self.state=self.expect_first_block_sequence_item\n  \n def expect_first_block_sequence_item(self):\n  return self.expect_block_sequence_item(first=True )\n  \n def expect_block_sequence_item(self,first=False ):\n  if not first and isinstance(self.event,SequenceEndEvent):\n   self.indent=self.indents.pop()\n   self.state=self.states.pop()\n  else :\n   self.write_indent()\n   self.write_indicator('-',True ,indention=True )\n   self.states.append(self.expect_block_sequence_item)\n   self.expect_node(sequence=True )\n   \n   \n   \n def expect_block_mapping(self):\n  self.increase_indent(flow=False )\n  self.state=self.expect_first_block_mapping_key\n  \n def expect_first_block_mapping_key(self):\n  return self.expect_block_mapping_key(first=True )\n  \n def expect_block_mapping_key(self,first=False ):\n  if not first and isinstance(self.event,MappingEndEvent):\n   self.indent=self.indents.pop()\n   self.state=self.states.pop()\n  else :\n   self.write_indent()\n   if self.check_simple_key():\n    self.states.append(self.expect_block_mapping_simple_value)\n    self.expect_node(mapping=True ,simple_key=True )\n   else :\n    self.write_indicator('?',True ,indention=True )\n    self.states.append(self.expect_block_mapping_value)\n    self.expect_node(mapping=True )\n    \n def expect_block_mapping_simple_value(self):\n  self.write_indicator(':',False )\n  self.states.append(self.expect_block_mapping_key)\n  self.expect_node(mapping=True )\n  \n def expect_block_mapping_value(self):\n  self.write_indent()\n  self.write_indicator(':',True ,indention=True )\n  self.states.append(self.expect_block_mapping_key)\n  self.expect_node(mapping=True )\n  \n  \n  \n def check_empty_sequence(self):\n  return (isinstance(self.event,SequenceStartEvent)and self.events\n  and isinstance(self.events[0],SequenceEndEvent))\n  \n def check_empty_mapping(self):\n  return (isinstance(self.event,MappingStartEvent)and self.events\n  and isinstance(self.events[0],MappingEndEvent))\n  \n def check_empty_document(self):\n  if not isinstance(self.event,DocumentStartEvent)or not self.events:\n   return False\n  event=self.events[0]\n  return (isinstance(event,ScalarEvent)and event.anchor is None\n  and event.tag is None and event.implicit and event.value =='')\n  \n def check_simple_key(self):\n  length=0\n  if isinstance(self.event,NodeEvent)and self.event.anchor is not None :\n   if self.prepared_anchor is None :\n    self.prepared_anchor=self.prepare_anchor(self.event.anchor)\n   length +=len(self.prepared_anchor)\n  if isinstance(self.event,(ScalarEvent,CollectionStartEvent))\\\n  and self.event.tag is not None :\n   if self.prepared_tag is None :\n    self.prepared_tag=self.prepare_tag(self.event.tag)\n   length +=len(self.prepared_tag)\n  if isinstance(self.event,ScalarEvent):\n   if self.analysis is None :\n    self.analysis=self.analyze_scalar(self.event.value)\n   length +=len(self.analysis.scalar)\n  return (length <128 and (isinstance(self.event,AliasEvent)\n  or (isinstance(self.event,ScalarEvent)\n  and not self.analysis.empty and not self.analysis.multiline)\n  or self.check_empty_sequence()or self.check_empty_mapping()))\n  \n  \n  \n def process_anchor(self,indicator):\n  if self.event.anchor is None :\n   self.prepared_anchor=None\n   return\n  if self.prepared_anchor is None :\n   self.prepared_anchor=self.prepare_anchor(self.event.anchor)\n  if self.prepared_anchor:\n   self.write_indicator(indicator+self.prepared_anchor,True )\n  self.prepared_anchor=None\n  \n def process_tag(self):\n  tag=self.event.tag\n  if isinstance(self.event,ScalarEvent):\n   if self.style is None :\n    self.style=self.choose_scalar_style()\n   if ((not self.canonical or tag is None )and\n   ((self.style ==''and self.event.implicit[0])\n   or (self.style !=''and self.event.implicit[1]))):\n    self.prepared_tag=None\n    return\n   if self.event.implicit[0]and tag is None :\n    tag='!'\n    self.prepared_tag=None\n  else :\n   if (not self.canonical or tag is None )and self.event.implicit:\n    self.prepared_tag=None\n    return\n  if tag is None :\n   raise EmitterError(\"tag is not specified\")\n  if self.prepared_tag is None :\n   self.prepared_tag=self.prepare_tag(tag)\n  if self.prepared_tag:\n   self.write_indicator(self.prepared_tag,True )\n  self.prepared_tag=None\n  \n def choose_scalar_style(self):\n  if self.analysis is None :\n   self.analysis=self.analyze_scalar(self.event.value)\n  if self.event.style =='\"'or self.canonical:\n   return '\"'\n  if not self.event.style and self.event.implicit[0]:\n   if (not (self.simple_key_context and\n   (self.analysis.empty or self.analysis.multiline))\n   and (self.flow_level and self.analysis.allow_flow_plain\n   or (not self.flow_level and self.analysis.allow_block_plain))):\n    return ''\n  if self.event.style and self.event.style in '|>':\n   if (not self.flow_level and not self.simple_key_context\n   and self.analysis.allow_block):\n    return self.event.style\n  if not self.event.style or self.event.style =='\\'':\n   if (self.analysis.allow_single_quoted and\n   not (self.simple_key_context and self.analysis.multiline)):\n    return '\\''\n  return '\"'\n  \n def process_scalar(self):\n  if self.analysis is None :\n   self.analysis=self.analyze_scalar(self.event.value)\n  if self.style is None :\n   self.style=self.choose_scalar_style()\n  split=(not self.simple_key_context)\n  \n  \n  \n  if self.style =='\"':\n   self.write_double_quoted(self.analysis.scalar,split)\n  elif self.style =='\\'':\n   self.write_single_quoted(self.analysis.scalar,split)\n  elif self.style =='>':\n   self.write_folded(self.analysis.scalar)\n  elif self.style =='|':\n   self.write_literal(self.analysis.scalar)\n  else :\n   self.write_plain(self.analysis.scalar,split)\n  self.analysis=None\n  self.style=None\n  \n  \n  \n def prepare_version(self,version):\n  major,minor=version\n  if major !=1:\n   raise EmitterError(\"unsupported YAML version: %d.%d\"%(major,minor))\n  return '%d.%d'%(major,minor)\n  \n def prepare_tag_handle(self,handle):\n  if not handle:\n   raise EmitterError(\"tag handle must not be empty\")\n  if handle[0]!='!'or handle[-1]!='!':\n   raise EmitterError(\"tag handle must start and end with '!': %r\"%handle)\n  for ch in handle[1:-1]:\n   if not ('0'<=ch <='9'or 'A'<=ch <='Z'or 'a'<=ch <='z'\\\n   or ch in '-_'):\n    raise EmitterError(\"invalid character %r in the tag handle: %r\"\n    %(ch,handle))\n  return handle\n  \n def prepare_tag_prefix(self,prefix):\n  if not prefix:\n   raise EmitterError(\"tag prefix must not be empty\")\n  chunks=[]\n  start=end=0\n  if prefix[0]=='!':\n   end=1\n  while end <len(prefix):\n   ch=prefix[end]\n   if '0'<=ch <='9'or 'A'<=ch <='Z'or 'a'<=ch <='z'\\\n   or ch in '-;/?!:@&=+$,_.~*\\'()[]':\n    end +=1\n   else :\n    if start <end:\n     chunks.append(prefix[start:end])\n    start=end=end+1\n    data=ch.encode('utf-8')\n    for ch in data:\n     chunks.append('%%%02X'%ord(ch))\n  if start <end:\n   chunks.append(prefix[start:end])\n  return ''.join(chunks)\n  \n def prepare_tag(self,tag):\n  if not tag:\n   raise EmitterError(\"tag must not be empty\")\n  if tag =='!':\n   return tag\n  handle=None\n  suffix=tag\n  prefixes=sorted(self.tag_prefixes.keys())\n  for prefix in prefixes:\n   if tag.startswith(prefix)\\\n   and (prefix =='!'or len(prefix)<len(tag)):\n    handle=self.tag_prefixes[prefix]\n    suffix=tag[len(prefix):]\n  chunks=[]\n  start=end=0\n  while end <len(suffix):\n   ch=suffix[end]\n   if '0'<=ch <='9'or 'A'<=ch <='Z'or 'a'<=ch <='z'\\\n   or ch in '-;/?:@&=+$,_.~*\\'()[]'\\\n   or (ch =='!'and handle !='!'):\n    end +=1\n   else :\n    if start <end:\n     chunks.append(suffix[start:end])\n    start=end=end+1\n    data=ch.encode('utf-8')\n    for ch in data:\n     chunks.append('%%%02X'%ch)\n  if start <end:\n   chunks.append(suffix[start:end])\n  suffix_text=''.join(chunks)\n  if handle:\n   return '%s%s'%(handle,suffix_text)\n  else :\n   return '!<%s>'%suffix_text\n   \n def prepare_anchor(self,anchor):\n  if not anchor:\n   raise EmitterError(\"anchor must not be empty\")\n  for ch in anchor:\n   if not ('0'<=ch <='9'or 'A'<=ch <='Z'or 'a'<=ch <='z'\\\n   or ch in '-_'):\n    raise EmitterError(\"invalid character %r in the anchor: %r\"\n    %(ch,anchor))\n  return anchor\n  \n def analyze_scalar(self,scalar):\n \n \n  if not scalar:\n   return ScalarAnalysis(scalar=scalar,empty=True ,multiline=False ,\n   allow_flow_plain=False ,allow_block_plain=True ,\n   allow_single_quoted=True ,allow_double_quoted=True ,\n   allow_block=False )\n   \n   \n  block_indicators=False\n  flow_indicators=False\n  line_breaks=False\n  special_characters=False\n  \n  \n  leading_space=False\n  leading_break=False\n  trailing_space=False\n  trailing_break=False\n  break_space=False\n  space_break=False\n  \n  \n  if scalar.startswith('---')or scalar.startswith('...'):\n   block_indicators=True\n   flow_indicators=True\n   \n   \n  preceded_by_whitespace=True\n  \n  \n  followed_by_whitespace=(len(scalar)==1 or\n  scalar[1]in '\\0 \\t\\r\\n\\x85\\u2028\\u2029')\n  \n  \n  previous_space=False\n  \n  \n  previous_break=False\n  \n  index=0\n  while index <len(scalar):\n   ch=scalar[index]\n   \n   \n   if index ==0:\n   \n    if ch in '#,[]{}&*!|>\\'\\\"%@`':\n     flow_indicators=True\n     block_indicators=True\n    if ch in '?:':\n     flow_indicators=True\n     if followed_by_whitespace:\n      block_indicators=True\n    if ch =='-'and followed_by_whitespace:\n     flow_indicators=True\n     block_indicators=True\n   else :\n   \n    if ch in ',?[]{}':\n     flow_indicators=True\n    if ch ==':':\n     flow_indicators=True\n     if followed_by_whitespace:\n      block_indicators=True\n    if ch =='#'and preceded_by_whitespace:\n     flow_indicators=True\n     block_indicators=True\n     \n     \n   if ch in '\\n\\x85\\u2028\\u2029':\n    line_breaks=True\n   if not (ch =='\\n'or '\\x20'<=ch <='\\x7E'):\n    if (ch =='\\x85'or '\\xA0'<=ch <='\\uD7FF'\n    or '\\uE000'<=ch <='\\uFFFD'\n    or '\\U00010000'<=ch <'\\U0010ffff')and ch !='\\uFEFF':\n     unicode_characters=True\n     if not self.allow_unicode:\n      special_characters=True\n    else :\n     special_characters=True\n     \n     \n   if ch ==' ':\n    if index ==0:\n     leading_space=True\n    if index ==len(scalar)-1:\n     trailing_space=True\n    if previous_break:\n     break_space=True\n    previous_space=True\n    previous_break=False\n   elif ch in '\\n\\x85\\u2028\\u2029':\n    if index ==0:\n     leading_break=True\n    if index ==len(scalar)-1:\n     trailing_break=True\n    if previous_space:\n     space_break=True\n    previous_space=False\n    previous_break=True\n   else :\n    previous_space=False\n    previous_break=False\n    \n    \n   index +=1\n   preceded_by_whitespace=(ch in '\\0 \\t\\r\\n\\x85\\u2028\\u2029')\n   followed_by_whitespace=(index+1 >=len(scalar)or\n   scalar[index+1]in '\\0 \\t\\r\\n\\x85\\u2028\\u2029')\n   \n   \n  allow_flow_plain=True\n  allow_block_plain=True\n  allow_single_quoted=True\n  allow_double_quoted=True\n  allow_block=True\n  \n  \n  if (leading_space or leading_break\n  or trailing_space or trailing_break):\n   allow_flow_plain=allow_block_plain=False\n   \n   \n  if trailing_space:\n   allow_block=False\n   \n   \n   \n  if break_space:\n   allow_flow_plain=allow_block_plain=allow_single_quoted=False\n   \n   \n   \n  if space_break or special_characters:\n   allow_flow_plain=allow_block_plain=\\\n   allow_single_quoted=allow_block=False\n   \n   \n   \n  if line_breaks:\n   allow_flow_plain=allow_block_plain=False\n   \n   \n  if flow_indicators:\n   allow_flow_plain=False\n   \n   \n  if block_indicators:\n   allow_block_plain=False\n   \n  return ScalarAnalysis(scalar=scalar,\n  empty=False ,multiline=line_breaks,\n  allow_flow_plain=allow_flow_plain,\n  allow_block_plain=allow_block_plain,\n  allow_single_quoted=allow_single_quoted,\n  allow_double_quoted=allow_double_quoted,\n  allow_block=allow_block)\n  \n  \n  \n def flush_stream(self):\n  if hasattr(self.stream,'flush'):\n   self.stream.flush()\n   \n def write_stream_start(self):\n \n  if self.encoding and self.encoding.startswith('utf-16'):\n   self.stream.write('\\uFEFF'.encode(self.encoding))\n   \n def write_stream_end(self):\n  self.flush_stream()\n  \n def write_indicator(self,indicator,need_whitespace,\n whitespace=False ,indention=False ):\n  if self.whitespace or not need_whitespace:\n   data=indicator\n  else :\n   data=' '+indicator\n  self.whitespace=whitespace\n  self.indention=self.indention and indention\n  self.column +=len(data)\n  self.open_ended=False\n  if self.encoding:\n   data=data.encode(self.encoding)\n  self.stream.write(data)\n  \n def write_indent(self):\n  indent=self.indent or 0\n  if not self.indention or self.column >indent\\\n  or (self.column ==indent and not self.whitespace):\n   self.write_line_break()\n  if self.column <indent:\n   self.whitespace=True\n   data=' '*(indent -self.column)\n   self.column=indent\n   if self.encoding:\n    data=data.encode(self.encoding)\n   self.stream.write(data)\n   \n def write_line_break(self,data=None ):\n  if data is None :\n   data=self.best_line_break\n  self.whitespace=True\n  self.indention=True\n  self.line +=1\n  self.column=0\n  if self.encoding:\n   data=data.encode(self.encoding)\n  self.stream.write(data)\n  \n def write_version_directive(self,version_text):\n  data='%%YAML %s'%version_text\n  if self.encoding:\n   data=data.encode(self.encoding)\n  self.stream.write(data)\n  self.write_line_break()\n  \n def write_tag_directive(self,handle_text,prefix_text):\n  data='%%TAG %s %s'%(handle_text,prefix_text)\n  if self.encoding:\n   data=data.encode(self.encoding)\n  self.stream.write(data)\n  self.write_line_break()\n  \n  \n  \n def write_single_quoted(self,text,split=True ):\n  self.write_indicator('\\'',True )\n  spaces=False\n  breaks=False\n  start=end=0\n  while end <=len(text):\n   ch=None\n   if end <len(text):\n    ch=text[end]\n   if spaces:\n    if ch is None or ch !=' ':\n     if start+1 ==end and self.column >self.best_width and split\\\n     and start !=0 and end !=len(text):\n      self.write_indent()\n     else :\n      data=text[start:end]\n      self.column +=len(data)\n      if self.encoding:\n       data=data.encode(self.encoding)\n      self.stream.write(data)\n     start=end\n   elif breaks:\n    if ch is None or ch not in '\\n\\x85\\u2028\\u2029':\n     if text[start]=='\\n':\n      self.write_line_break()\n     for br in text[start:end]:\n      if br =='\\n':\n       self.write_line_break()\n      else :\n       self.write_line_break(br)\n     self.write_indent()\n     start=end\n   else :\n    if ch is None or ch in ' \\n\\x85\\u2028\\u2029'or ch =='\\'':\n     if start <end:\n      data=text[start:end]\n      self.column +=len(data)\n      if self.encoding:\n       data=data.encode(self.encoding)\n      self.stream.write(data)\n      start=end\n   if ch =='\\'':\n    data='\\'\\''\n    self.column +=2\n    if self.encoding:\n     data=data.encode(self.encoding)\n    self.stream.write(data)\n    start=end+1\n   if ch is not None :\n    spaces=(ch ==' ')\n    breaks=(ch in '\\n\\x85\\u2028\\u2029')\n   end +=1\n  self.write_indicator('\\'',False )\n  \n ESCAPE_REPLACEMENTS={\n '\\0':'0',\n '\\x07':'a',\n '\\x08':'b',\n '\\x09':'t',\n '\\x0A':'n',\n '\\x0B':'v',\n '\\x0C':'f',\n '\\x0D':'r',\n '\\x1B':'e',\n '\\\"':'\\\"',\n '\\\\':'\\\\',\n '\\x85':'N',\n '\\xA0':'_',\n '\\u2028':'L',\n '\\u2029':'P',\n }\n \n def write_double_quoted(self,text,split=True ):\n  self.write_indicator('\"',True )\n  start=end=0\n  while end <=len(text):\n   ch=None\n   if end <len(text):\n    ch=text[end]\n   if ch is None or ch in '\"\\\\\\x85\\u2028\\u2029\\uFEFF'\\\n   or not ('\\x20'<=ch <='\\x7E'\n   or (self.allow_unicode\n   and ('\\xA0'<=ch <='\\uD7FF'\n   or '\\uE000'<=ch <='\\uFFFD'))):\n    if start <end:\n     data=text[start:end]\n     self.column +=len(data)\n     if self.encoding:\n      data=data.encode(self.encoding)\n     self.stream.write(data)\n     start=end\n    if ch is not None :\n     if ch in self.ESCAPE_REPLACEMENTS:\n      data='\\\\'+self.ESCAPE_REPLACEMENTS[ch]\n     elif ch <='\\xFF':\n      data='\\\\x%02X'%ord(ch)\n     elif ch <='\\uFFFF':\n      data='\\\\u%04X'%ord(ch)\n     else :\n      data='\\\\U%08X'%ord(ch)\n     self.column +=len(data)\n     if self.encoding:\n      data=data.encode(self.encoding)\n     self.stream.write(data)\n     start=end+1\n   if 0 <end <len(text)-1 and (ch ==' 'or start >=end)\\\n   and self.column+(end -start)>self.best_width and split:\n    data=text[start:end]+'\\\\'\n    if start <end:\n     start=end\n    self.column +=len(data)\n    if self.encoding:\n     data=data.encode(self.encoding)\n    self.stream.write(data)\n    self.write_indent()\n    self.whitespace=False\n    self.indention=False\n    if text[start]==' ':\n     data='\\\\'\n     self.column +=len(data)\n     if self.encoding:\n      data=data.encode(self.encoding)\n     self.stream.write(data)\n   end +=1\n  self.write_indicator('\"',False )\n  \n def determine_block_hints(self,text):\n  hints=''\n  if text:\n   if text[0]in ' \\n\\x85\\u2028\\u2029':\n    hints +=str(self.best_indent)\n   if text[-1]not in '\\n\\x85\\u2028\\u2029':\n    hints +='-'\n   elif len(text)==1 or text[-2]in '\\n\\x85\\u2028\\u2029':\n    hints +='+'\n  return hints\n  \n def write_folded(self,text):\n  hints=self.determine_block_hints(text)\n  self.write_indicator('>'+hints,True )\n  if hints[-1:]=='+':\n   self.open_ended=True\n  self.write_line_break()\n  leading_space=True\n  spaces=False\n  breaks=True\n  start=end=0\n  while end <=len(text):\n   ch=None\n   if end <len(text):\n    ch=text[end]\n   if breaks:\n    if ch is None or ch not in '\\n\\x85\\u2028\\u2029':\n     if not leading_space and ch is not None and ch !=' '\\\n     and text[start]=='\\n':\n      self.write_line_break()\n     leading_space=(ch ==' ')\n     for br in text[start:end]:\n      if br =='\\n':\n       self.write_line_break()\n      else :\n       self.write_line_break(br)\n     if ch is not None :\n      self.write_indent()\n     start=end\n   elif spaces:\n    if ch !=' ':\n     if start+1 ==end and self.column >self.best_width:\n      self.write_indent()\n     else :\n      data=text[start:end]\n      self.column +=len(data)\n      if self.encoding:\n       data=data.encode(self.encoding)\n      self.stream.write(data)\n     start=end\n   else :\n    if ch is None or ch in ' \\n\\x85\\u2028\\u2029':\n     data=text[start:end]\n     self.column +=len(data)\n     if self.encoding:\n      data=data.encode(self.encoding)\n     self.stream.write(data)\n     if ch is None :\n      self.write_line_break()\n     start=end\n   if ch is not None :\n    breaks=(ch in '\\n\\x85\\u2028\\u2029')\n    spaces=(ch ==' ')\n   end +=1\n   \n def write_literal(self,text):\n  hints=self.determine_block_hints(text)\n  self.write_indicator('|'+hints,True )\n  if hints[-1:]=='+':\n   self.open_ended=True\n  self.write_line_break()\n  breaks=True\n  start=end=0\n  while end <=len(text):\n   ch=None\n   if end <len(text):\n    ch=text[end]\n   if breaks:\n    if ch is None or ch not in '\\n\\x85\\u2028\\u2029':\n     for br in text[start:end]:\n      if br =='\\n':\n       self.write_line_break()\n      else :\n       self.write_line_break(br)\n     if ch is not None :\n      self.write_indent()\n     start=end\n   else :\n    if ch is None or ch in '\\n\\x85\\u2028\\u2029':\n     data=text[start:end]\n     if self.encoding:\n      data=data.encode(self.encoding)\n     self.stream.write(data)\n     if ch is None :\n      self.write_line_break()\n     start=end\n   if ch is not None :\n    breaks=(ch in '\\n\\x85\\u2028\\u2029')\n   end +=1\n   \n def write_plain(self,text,split=True ):\n  if self.root_context:\n   self.open_ended=True\n  if not text:\n   return\n  if not self.whitespace:\n   data=' '\n   self.column +=len(data)\n   if self.encoding:\n    data=data.encode(self.encoding)\n   self.stream.write(data)\n  self.whitespace=False\n  self.indention=False\n  spaces=False\n  breaks=False\n  start=end=0\n  while end <=len(text):\n   ch=None\n   if end <len(text):\n    ch=text[end]\n   if spaces:\n    if ch !=' ':\n     if start+1 ==end and self.column >self.best_width and split:\n      self.write_indent()\n      self.whitespace=False\n      self.indention=False\n     else :\n      data=text[start:end]\n      self.column +=len(data)\n      if self.encoding:\n       data=data.encode(self.encoding)\n      self.stream.write(data)\n     start=end\n   elif breaks:\n    if ch not in '\\n\\x85\\u2028\\u2029':\n     if text[start]=='\\n':\n      self.write_line_break()\n     for br in text[start:end]:\n      if br =='\\n':\n       self.write_line_break()\n      else :\n       self.write_line_break(br)\n     self.write_indent()\n     self.whitespace=False\n     self.indention=False\n     start=end\n   else :\n    if ch is None or ch in ' \\n\\x85\\u2028\\u2029':\n     data=text[start:end]\n     self.column +=len(data)\n     if self.encoding:\n      data=data.encode(self.encoding)\n     self.stream.write(data)\n     start=end\n   if ch is not None :\n    spaces=(ch ==' ')\n    breaks=(ch in '\\n\\x85\\u2028\\u2029')\n   end +=1\n", ["yaml.error", "yaml.events"]]}
__BRYTHON__.update_VFS(scripts)
